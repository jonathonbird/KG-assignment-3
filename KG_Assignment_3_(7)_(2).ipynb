{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBhpnT7cvIX8",
        "outputId": "2dd8c670-ef08-479e-96d0-4a5d0c75e19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyAkb4H5oklh",
        "outputId": "8c5b4a3e-f36a-4546-bb64-0ce394d029d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_concepts:  516782  embedding dim:  300\n",
            "(516782, 300)\n",
            "516782\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "def load_embeddings(embeddings_zip, embeddings_file):\n",
        "  \"\"\"Loads embeddings from a Numberbatch file inside a zip archive.\n",
        "\n",
        "  Args:\n",
        "      embeddings_zip: Path to the 'numberbatch-en.zip' file.\n",
        "      embeddings_file: Name of the 'numberbatch-en.txt' file within the zip archive.\n",
        "\n",
        "  Returns:\n",
        "      A tuple of (embeddings, vocabulary)\n",
        "          embeddings: A NumPy array of shape (num_concepts, embedding_dim)\n",
        "          vocabulary: A list of concept labels in the order they appear in the embeddings array\n",
        "  \"\"\"\n",
        "\n",
        "  with zipfile.ZipFile(embeddings_zip, 'r') as zip_ref:\n",
        "    with zip_ref.open(embeddings_file) as f:\n",
        "      header = f.readline().decode('utf-8').split()  # Read dimensions\n",
        "      num_concepts, embedding_dim = int(header[0]), int(header[1])\n",
        "      print(\"num_concepts: \", num_concepts, \" embedding dim: \", embedding_dim)\n",
        "\n",
        "      embeddings = np.zeros((num_concepts, embedding_dim))\n",
        "      vocabulary = []\n",
        "\n",
        "      for i, line in enumerate(f):\n",
        "        parts = line.decode('utf-8').strip().split()  # Decode bytes\n",
        "        vocabulary.append(parts[0])\n",
        "\n",
        "        try:\n",
        "          embeddings[i] = np.array(parts[1:], dtype=np.float32)\n",
        "        except ValueError as e:\n",
        "          print(f\"Error on line {i+2}: {line}\")\n",
        "          print(f\"Error message: {e}\")\n",
        "\n",
        "  return embeddings, vocabulary\n",
        "\n",
        "# Load the embeddings and vocabulary\n",
        "embeddings, vocabulary = load_embeddings(\"/content/drive/My Drive/WikiData/numberbatch-en.zip\", \"numberbatch-en.txt\")\n",
        "print(embeddings.shape)\n",
        "print(len(vocabulary))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FvCwmMsydkhL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "7ecb8779bbb94ba883c4c6fbd28cc2d8",
            "9bb6669d3fe3400e8bb5db37a495cbb6",
            "6e4165dd23684694b627ffa20dd24877",
            "a0f60da9b416494cbdeeff225967a0a1",
            "33f7a97c49b04810ad25e5e2f2042376",
            "f66425a4e589462ab15a2ca575e2fb34",
            "d15e687e17984d31b4edb1cb0d844533",
            "0219b2dda8b347108da726f092ae4932",
            "33d969cdfbd34a72950c5467a986b6da",
            "82da9a74216944a495d6eb495b4aa4ab",
            "2f84e91d52034ea59d9ab6fd34b1877e",
            "d1e52fa437ce4a909535c968f433e3e4",
            "63e10ddaafb04328911c311da0e47253",
            "574167f653b4416fae42f0bcc94d350b",
            "842d640f51e645c9b51ca168bc901f4e",
            "cf444aa224f74b5fbefc7f07f6c8af59",
            "388ba9f96d234418949a06dedebc0d33",
            "51a97f917b2e4460898bbdbf74f1187a",
            "38248d6be2104ebeaf16f65db27cf91f",
            "15276ec0c4964e2a84c66bcc99fb1b6c",
            "6c4f6ff5070843ec8b57e3227dfbf3eb",
            "3603428fb61148ef858439229d2d03e6",
            "9c1352e83dd740c59e2cd341762db5bc",
            "9251a55054ac4bbbbe7b00fb91086862",
            "28bd3c39e0b84a01972f6336cbca0f8c",
            "bb9ac41a30c444f2993dd7b9dc25c6c8",
            "e3eaf0ab17bf47ce94c51827c9742fa5",
            "d8c72b603b0a4f939f0dbcf1b0900fcf",
            "e0f14474d77649e5b51d2ed48e0a17bc",
            "74ef5c39d95a4ef98a5fa870a997be57",
            "704711b3453746878b1970139c6f3a41",
            "c994b44bfc924a1cbe7dfc83448df189",
            "b19c88f3e6564b7eb434adb218a475f7",
            "3a7ec4560e934f498395ac3fe9caf20f",
            "57d92d8b3a1044a9a4e333305a21d257",
            "ad5b917e93f94bca80df5a7dccdc9006",
            "376ab5a579894e24a99acb1e0c35de72",
            "8c1e4e565e3c4a9c961bb0c4f0352142",
            "61f5040ba802493abceb77e7e3ae1527",
            "6034673414984180a39be60f51d48047",
            "c11ad45336c0447787e96de64f749bbe",
            "52e137360568400398b49076bf066954",
            "a338674f9a8841b5a2ff85dd9f574637",
            "5bf72b6152ee4b5fa1f944a55b2a9278",
            "b135076b2331409e9071d2980e6c5ce6",
            "db2b454548e0474aa2c14c1945762436",
            "1de095d6f91049d889502d85c2765721",
            "224a7b23b1564bf5a1f072f78da018ba",
            "dde637bb9d1043bebbf9648eac1524c5",
            "1cfa349d089d43b9954fdf536d6c86dc",
            "dee1c15d77154add91e3517209a20317",
            "3ce1698ce20944f1b9be4ea2b87cc1e9",
            "648260ad5bcc451ca3fdeddf5d5e530c",
            "01a7dafe10f844779afd609de299a8ff",
            "875aad3566a445f39173a078a9cbfbb0"
          ]
        },
        "outputId": "01c12893-e6a7-4f95-ac96-cb0d378d5daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ecb8779bbb94ba883c4c6fbd28cc2d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1e52fa437ce4a909535c968f433e3e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c1352e83dd740c59e2cd341762db5bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a7ec4560e934f498395ac3fe9caf20f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b135076b2331409e9071d2980e6c5ce6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # Or another suitable BERT model\n",
        "model1 = AutoModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADEO79z_3xqE"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "conceptnet_data = {}\n",
        "with gzip.open(\"/content/drive/My Drive/WikiData/conceptnet-assertions-5.7.0.csv.gz\", 'rt') as f:  # 'rt' for reading text\n",
        "    for line in f:\n",
        "        _, relation, start_concept, end_concept, extra_info = line.split('\\t')\n",
        "        extra_info_dict = json.loads(extra_info)  # Load JSON from extra_info\n",
        "\n",
        "        if 'surfaceText' in extra_info_dict:\n",
        "            surface_text = extra_info_dict['surfaceText']\n",
        "\n",
        "            if start_concept not in conceptnet_data:\n",
        "                 conceptnet_data[start_concept] = []\n",
        "            if surface_text != None and surface_text!=\"null\":\n",
        "              conceptnet_data[start_concept].append(surface_text)  # Store the surface text\n",
        "            # Include sense_label\n",
        "            if 'sense_label' in extra_info_dict:\n",
        "                sense_label = extra_info_dict['sense_label']\n",
        "            else:\n",
        "                sense_label = None  # Handle cases where it might be absent\n",
        "            if 'label' in extra_info_dict:\n",
        "                label = extra_info_dict['label']\n",
        "            else:\n",
        "                label = None\n",
        "            if label != None and label!=\"null\":\n",
        "                conceptnet_data[start_concept].append(label)\n",
        "            if sense_label != None and sense_label!=\"null\":\n",
        "                conceptnet_data[start_concept].append(sense_label)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3jn7KlG7BmL"
      },
      "outputs": [],
      "source": [
        "test = list(conceptnet_data.keys())[50:60]\n",
        "print(len(list(conceptnet_data.keys())))\n",
        "\n",
        "for t in test:\n",
        "  print(t,\": \",conceptnet_data[t])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X0ybJ5S_qo1"
      },
      "outputs": [],
      "source": [
        "print(vocabulary.index(\"able\"))\n",
        "\n",
        "s= \"/c/en/able\"\n",
        "\n",
        "print(s[6:])\n",
        "print(s[6])\n",
        "print(s[:6])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_lookup = {concept_label: idx for idx, concept_label in enumerate(vocabulary)}"
      ],
      "metadata": {
        "id": "16Joa4k_1Fxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX22RcmeKwbt"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import requests\n",
        "import torch\n",
        "import pandas as pd\n",
        "def get_concept_description(concept_label):\n",
        "    if concept_label in conceptnet_data:\n",
        "        return conceptnet_data[concept_label]\n",
        "    else:\n",
        "        print(\"No descriptions found for:\", concept_label)\n",
        "        return []\n",
        "\n",
        "# Create an empty list to store the data for the dataframe\n",
        "data = []\n",
        "\n",
        "\n",
        "i=0\n",
        "# Iterate directly through conceptnet_data\n",
        "for concept_label, descriptions in conceptnet_data.items():\n",
        "    if concept_label.split('/')[1]!='en':\n",
        "      continue\n",
        "    if descriptions:\n",
        "        try:\n",
        "            embedding = embeddings[embedding_lookup[concept_label.split('/')[2]]]\n",
        "        except:\n",
        "            print(\"ERROR: \",concept_label)\n",
        "            embedding = None  # Handle cases where concept_label might not be in 'vocabulary'\n",
        "            continue\n",
        "        i=i+1\n",
        "        if i%250==0:\n",
        "          print(i,\": \",concept_label)\n",
        "        text_embeddings = []\n",
        "        for description in descriptions:\n",
        "            inputs = tokenizer(description, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = model1(**inputs)\n",
        "                token_embeddings = outputs.last_hidden_state[:,0,:]\n",
        "            text_embeddings.append(token_embeddings)\n",
        "        data.append({'text': text_embeddings, 'embedding': embedding})\n",
        "# Create the pandas dataframe\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the dataframe\n",
        "print(df)\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJlv_K4O-V-F"
      },
      "outputs": [],
      "source": [
        "# Save the dataframe as a CSV file\n",
        "df.to_csv(\"concept_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"concept_embeddings.csv\")\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "SCqldoulXTq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BM2ZsM9NfybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QgDBREuD1dAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import requests\n",
        "import torch\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "df = pd.read_csv('/content/drive/My Drive/WikiData/concept_embeddings (2).csv')"
      ],
      "metadata": {
        "id": "MuKHxbRLowEH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def convert_tensor_string(tensor_str):\n",
        "    # Remove unnecessary characters\n",
        "    clean_str = re.sub(r\"\\[|\\]\", \"\", tensor_str)\n",
        "\n",
        "    # Split values into a list of strings\n",
        "    str_values = clean_str.split()\n",
        "\n",
        "    # Convert strings to floats and create a tensor\n",
        "    float_values = [float(val) for val in str_values]\n",
        "    tensor = torch.tensor(float_values)\n",
        "\n",
        "    return tensor\n",
        "df['embedding'] = df['embedding'].apply(convert_tensor_string)\n"
      ],
      "metadata": {
        "id": "7TrwhMiEsdmW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import re\n",
        "\n",
        "\n",
        "def extract_tensors(tensor_string):\n",
        "    # Preprocessing: Remove extra characters if necessary\n",
        "    clean_string = tensor_string.strip()\n",
        "\n",
        "    # Split by a reliable delimiter\n",
        "    tensor_substrings = clean_string.split(\"tensor(\")[1:]\n",
        "\n",
        "    tensors = []\n",
        "    for substring in tensor_substrings:\n",
        "        # Remove brackets and split into numbers\n",
        "        clean_values = substring.strip(\"[])\\n\").split(\",\")\n",
        "        clean_values = [val.strip() for val in clean_values]  # Remove leading/trailing spaces\n",
        "        clean_values = [val.rstrip(\"])\") for val in clean_values]  # Remove extra \"])\"\n",
        "        float_values = []\n",
        "        for val in clean_values:\n",
        "          if val != \"\":\n",
        "            float_values.append(float(val))\n",
        "        # Convert to floats and create a tensor\n",
        "        tensor = torch.tensor(float_values)\n",
        "        tensors.append(tensor)\n",
        "\n",
        "    # Create the final tensor of tensors\n",
        "    tensor_of_tensors = torch.stack(tensors)\n",
        "    return tensor_of_tensors\n",
        "\n",
        "df['text'] = df['text'].apply(extract_tensors)"
      ],
      "metadata": {
        "id": "UmZodVpdthff"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('processed_data.pkl', 'wb') as f:\n",
        "    pickle.dump(df, f)"
      ],
      "metadata": {
        "id": "VDoLvIuB7dt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import requests\n",
        "import torch\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "with open('processed_data.pkl', 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "# Access your data (tensors are already in the correct format)\n",
        "embeddings = df['embedding'].values\n",
        "print(embeddings[0])  # Access an embedding tensor directly"
      ],
      "metadata": {
        "id": "ScRcP37zp5d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "del model\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "Kg11HUeFlK5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available: \", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"CUDA is not available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdENbw1SE9Ti",
        "outputId": "622df90c-bc7b-48e1-b74e-d6f268b5f4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available:  NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9IgDjIJnpIKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, BertTokenizer\n",
        "device = torch.device(\"cuda\")\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear4 = nn.Linear(hidden_size, latent_size)\n",
        "        self.linear_mu = nn.Linear(latent_size, latent_size)\n",
        "        self.linear_logvar = nn.Linear(latent_size, latent_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"encoder1: \",x.shape)\n",
        "        out = torch.relu(self.linear1(x))\n",
        "        #print(\"encoder2: \",out.shape)\n",
        "        out = torch.relu(self.linear2(out))\n",
        "        out = torch.relu(self.linear3(out))\n",
        "        out = torch.relu(self.linear4(out))\n",
        "        #print(\"encoder3: \",out.shape)\n",
        "        mu = self.linear_mu(out)\n",
        "        logvar = self.linear_logvar(out)\n",
        "\n",
        "        # Reparameterization trick for backpropagation through sampling\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = eps.mul(std).add_(mu)  # Sample from the latent distribution\n",
        "        return z, mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_size, decoder_hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(latent_size, decoder_hidden_size)\n",
        "        self.linear2 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.output_layer = nn.Linear(decoder_hidden_size, output_size)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #print(\"decoder 1: \",z.shape)\n",
        "        #print(\"decoder 1: \",z)\n",
        "        out = torch.relu(self.linear1(z))\n",
        "        out = torch.relu(self.linear2(out))\n",
        "        out = self.output_layer(out)\n",
        "       # print(\"decoder 2: \",out)\n",
        "        return out\n",
        "\n",
        "class SentenceAttention(nn.Module):\n",
        "    def __init__(self, attention_hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(attention_hidden_size, attention_hidden_size)\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "      # Transform x using a linear layer; output shape will be (sq, b, hidden_size)\n",
        "      x_transformed = self.linear(encoder_outputs)\n",
        "      # Step 2: Compute attention scores using softmax across the sequence dimension (sq)\n",
        "      # Attention scores shape: (sq, b, hidden_size) -> (b, sq, hidden_size) for softmax\n",
        "      x_transposed = x_transformed.transpose(0, 1)  # Transposing for softmax operation\n",
        "      attention_scores = F.softmax(x_transposed, dim=1)  # Applying softmax; shape remains (b, sq, hidden_size)\n",
        "\n",
        "      # Step 3: Apply attention scores to the original input tensor\n",
        "      # For weighted sum, first transpose x back: (sq, b, hidden_size) -> (b, sq, hidden_size)\n",
        "      x = encoder_outputs.transpose(0, 1)  # Transposing x to match attention_scores shape\n",
        "      # Compute the context vector as the weighted sum of the input vectors\n",
        "      # (b, sq, hidden_size) * (b, sq, hidden_size) -> (b, hidden_size) after summing over sq dimension\n",
        "      context_vector = torch.sum(attention_scores * x, dim=1)\n",
        "      return context_vector\n",
        "\n",
        "class TextVAE(nn.Module):\n",
        "    def __init__(self, bert_embedding_size, hidden_size, latent_size):\n",
        "        super().__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=bert_embedding_size,\n",
        "            nhead=8,  # Number of attention heads\n",
        "            dim_feedforward=hidden_size, # Dim. of feedforward network\n",
        "            batch_first=False\n",
        "        )\n",
        "        self.bert_size = bert_embedding_size\n",
        "        # Stack Multiple Transformer Layers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=5)\n",
        "        self.encoder = Encoder(bert_embedding_size, hidden_size, latent_size)\n",
        "        self.decoder = Decoder(latent_size, hidden_size, bert_embedding_size)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=bert_embedding_size,\n",
        "            nhead=8,\n",
        "            dim_feedforward=hidden_size,\n",
        "            batch_first=False\n",
        "        )\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=5)\n",
        "        self.z=None\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        out = self.transformer_encoder(text_embeddings)\n",
        "        attention_layer = SentenceAttention(self.bert_size).to(device)  # Initialize attention\n",
        "        context = attention_layer(out)  # Apply attention\n",
        "        # Encoding and Decoding\n",
        "        z, mu, logvar = self.encoder(context)\n",
        "        text_recon = self.decoder(z)\n",
        "        output = self.transformer_decoder(text_recon, context)  # Transformer for final output embeddings\n",
        "        self.z=z\n",
        "        return output, z, mu, logvar\n",
        "\n",
        "\n",
        "class KGVAE(nn.Module):\n",
        "    def __init__(self, kg_embedding_size, hidden_size, latent_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(kg_embedding_size, hidden_size, latent_size)\n",
        "        self.decoder = Decoder(latent_size, hidden_size, kg_embedding_size)\n",
        "        self.z=None\n",
        "\n",
        "    def forward(self, kg_embeddings):\n",
        "        z, mu, logvar = self.encoder(kg_embeddings)\n",
        "        kg_recon = self.decoder(z)\n",
        "        self.z=z\n",
        "        return kg_recon, z, mu, logvar\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size, hidden_size):\n",
        "        super().__init__()\n",
        "        noise_dim = latent_size\n",
        "        self.linear1 = nn.Linear(latent_size, hidden_size)\n",
        "        self.bn1 = ConditionalBatchNorm1d(hidden_size, latent_size)  # Conditional Batch\n",
        "       # self.linear2 = nn.Linear(latent_size, hidden_size)\n",
        "      #  self.bn2 = ConditionalBatchNorm1d(hidden_size, latent_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, latent_size)\n",
        "\n",
        "    def forward(self, text_z, noise):\n",
        "        out = torch.relu(self.linear1(noise))  # Process noise directly\n",
        "        out = self.bn1(out, text_z)\n",
        "\n",
        "       # out = torch.relu(self.linear2(noise))\n",
        "       # out = self.bn2(out, text_z)\n",
        "\n",
        "        fake_kg_z = self.output_layer(out)\n",
        "        return fake_kg_z\n",
        "\n",
        "\n",
        "class ConditionalBatchNorm1d(nn.Module):\n",
        "    def __init__(self, num_features, num_conditions):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "\n",
        "        # Learnable scale and bias parameters, conditioned on the input 'condition'\n",
        "        self.gamma_layer = nn.Linear(num_conditions, self.num_features)\n",
        "        self.beta_layer = nn.Linear(num_conditions, self.num_features)\n",
        "\n",
        "    def forward(self, input, condition):\n",
        "\n",
        "        out = F.batch_norm(input, None, None, training=True)  # Standard batch normalization\n",
        "        #print(\"out: \",out.shape)\n",
        "        #print(\"condition: \",condition.shape)\n",
        "        condition_expanded = condition.unsqueeze(2).expand(-1, -1, self.num_features)\n",
        "        #print(\"condition ex: \",condition_expanded.shape)\n",
        "\n",
        "        # Calculate scale and bias\n",
        "        gamma = self.gamma_layer(condition)[:, :, None].squeeze(-1)\n",
        "        beta = self.beta_layer(condition)[:, :, None].squeeze(-1)\n",
        "\n",
        "        #print(\"gamma: \",gamma.shape)\n",
        "        #print(\"beta: \",beta.shape)\n",
        "\n",
        "        # Apply scaling and shifting\n",
        "        out = gamma * out + beta\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, kg_latent_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(kg_latent_size, 8)\n",
        "        self.linear2 = nn.Linear(8, 4)\n",
        "        self.output_layer = nn.Linear(4, 1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = torch.relu(self.linear1(z))\n",
        "        out = torch.relu(self.linear2(out))\n",
        "        out = self.output_layer(out)\n",
        "        return out  # Output for classification (real or fake)\n"
      ],
      "metadata": {
        "id": "WqXDKGsEhSum"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data.dataset as dataset\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "epochs = 10\n",
        "latent_size = 2\n",
        "batch_size= 32\n",
        "class TextVAEDataset(dataset.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text_embeddings = torch.cat([self.df.iloc[index]['text']]).float()\n",
        "        return text_embeddings  # Return only the text embeddings\n",
        "\n",
        "# Updated collate function (no need for kg_embeddings)\n",
        "def text_vae_collate_fn(batch):\n",
        "    \"\"\"Collates a batch of text embeddings for the TextVAE.\n",
        "    \"\"\"\n",
        "    if len(batch)!=batch_size:\n",
        "      return None\n",
        "    batch.sort(key=lambda x: len(x), reverse=True)\n",
        "    text_embeddings_padded = pad_sequence(batch, batch_first=False)\n",
        "    return text_embeddings_padded\n",
        "\n",
        "# Using the new dataset and collate function:\n",
        "dataset_textvae = TextVAEDataset(df)\n",
        "dataloader_textvae = DataLoader(\n",
        "    dataset_textvae,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=text_vae_collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "beta=1\n",
        "\n",
        "textModel = TextVAE(\n",
        "    bert_embedding_size=768,  # Adjust if using a different BERT model\n",
        "    hidden_size=256,\n",
        "    latent_size=latent_size   # Choose hyperparameters\n",
        ").to(device)\n",
        "optimizer = optim.Adam(textModel.parameters(), lr=0.0005)  # Adjust learning rate as needed\n",
        "\n",
        "def train_text_vae(model, dataloader, optimizer, num_epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for text_embeddings in dataloader:\n",
        "            if text_embeddings == None:\n",
        "              continue\n",
        "            text_embeddings = text_embeddings.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, text_z, text_mu, text_logvar = model(text_embeddings)\n",
        "            attention_layer = SentenceAttention(768).to(device)  # Initialize attention\n",
        "            context = attention_layer(text_embeddings)  # Apply attention\n",
        "            # Calculate reconstruction and KL-Divergence loss\n",
        "            # (you might need to adjust if a different loss is used)\n",
        "            text_rc_loss = F.mse_loss(output, context)\n",
        "            text_kl_loss = -0.5 * torch.sum(1 + text_logvar - text_mu.pow(2) - text_logvar.exp())\n",
        "\n",
        "            loss = text_rc_loss + 1.5 * text_kl_loss  # Beta for KL weighting\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch+1}: Training Loss - {avg_loss:.4f}')\n",
        "    return text_z, textModel.encoder\n",
        "\n",
        "# Start training\n",
        "z_text, textEncoder = train_text_vae(textModel, dataloader_textvae, optimizer, num_epochs = epochs)\n",
        "textModel.eval()\n",
        "z_text = z_text.detach().to(device)  # Break the computational graph\n",
        "del textModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUCnE2aptzE",
        "outputId": "d222870a-eb37-4e5c-93f0-b089310cc663"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Loss - 4.2762\n",
            "Epoch 2: Training Loss - 0.8016\n",
            "Epoch 3: Training Loss - 0.1266\n",
            "Epoch 4: Training Loss - 0.0216\n",
            "Epoch 5: Training Loss - 0.0123\n",
            "Epoch 6: Training Loss - 0.0122\n",
            "Epoch 7: Training Loss - 0.0120\n",
            "Epoch 8: Training Loss - 0.0122\n",
            "Epoch 9: Training Loss - 0.0122\n",
            "Epoch 10: Training Loss - 0.0120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KGVAEDataset(dataset.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Convert kg_embedding to a PyTorch tensor\n",
        "        kg_embedding = torch.tensor(self.df.iloc[index]['embedding']).float()\n",
        "        return kg_embedding\n",
        "\n",
        "def kg_vae_collate_fn(batch):\n",
        "    \"\"\"Collates a batch of KG embeddings for the KGVAE.\n",
        "    \"\"\"\n",
        "    if len(batch)!=batch_size:\n",
        "      return None\n",
        "    kg_embeddings = torch.stack(batch)\n",
        "    return kg_embeddings\n",
        "# Create the dataloader for KGVAE (reuse existing classes and functions)\n",
        "dataset_kgvae = KGVAEDataset(df)\n",
        "dataloader_kgvae = DataLoader(\n",
        "    dataset_kgvae,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=kg_vae_collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "def train_kg_vae(model, dataloader, optimizer, num_epochs=10):\n",
        "    model.train()  # Set the KGVAE model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for kg_embeddings in dataloader:\n",
        "            if kg_embeddings == None:\n",
        "              continue\n",
        "            kg_embeddings = kg_embeddings.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            kg_recon, kg_z, kg_mu, kg_logvar = model(kg_embeddings)\n",
        "            # Calculate reconstruction and KL-Divergence loss\n",
        "            # (you might need to adjust if a different loss is used)\n",
        "            kg_rc_loss = 1 - F.cosine_similarity(kg_recon, kg_embeddings, dim=-1).mean()\n",
        "            kg_kl_loss = -0.5 * torch.sum(1 + kg_logvar - kg_mu.pow(2) - kg_logvar.exp())\n",
        "\n",
        "            loss = kg_rc_loss + 1.5 * kg_kl_loss  # Beta for KL weighting\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch+1}: KGVAE Training Loss - {avg_loss:.4f}')\n",
        "    return kg_z, model_kgvae.decoder\n",
        "\n",
        "# Assuming you have your KGVAE model defined and a dataloader for KG embeddings\n",
        "model_kgvae = KGVAE(\n",
        "    kg_embedding_size=300,  # Adjust according to your KG embeddings\n",
        "    hidden_size=256,\n",
        "    latent_size=latent_size   # Choose hyperparameters\n",
        ").to(device)\n",
        "optimizer_kgvae = optim.Adam(model_kgvae.parameters(), lr=0.0005)  # Adjust learning rate as needed\n",
        "\n",
        "# Start training (after TextVAE has been trained)\n",
        "kg_z, kgDecoder = train_kg_vae(model_kgvae, dataloader_kgvae, optimizer_kgvae, num_epochs = epochs)\n",
        "model_kgvae.eval()\n",
        "kg_z = kg_z.detach().to(device)\n",
        "del model_kgvae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9BBR2uisD7K",
        "outputId": "921c60c3-e9af-4205-91ed-8b92d4763ef4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b9e862296f02>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  kg_embedding = torch.tensor(self.df.iloc[index]['embedding']).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: KGVAE Training Loss - 3.4457\n",
            "Epoch 2: KGVAE Training Loss - 0.2530\n",
            "Epoch 3: KGVAE Training Loss - 0.2527\n",
            "Epoch 4: KGVAE Training Loss - 0.2527\n",
            "Epoch 5: KGVAE Training Loss - 0.2528\n",
            "Epoch 6: KGVAE Training Loss - 0.2527\n",
            "Epoch 7: KGVAE Training Loss - 0.2527\n",
            "Epoch 8: KGVAE Training Loss - 0.2527\n",
            "Epoch 9: KGVAE Training Loss - 0.2526\n",
            "Epoch 10: KGVAE Training Loss - 0.2525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "# Initialize SentenceAttention model (assuming 768-dim embeddings)\n",
        "attention_model = SentenceAttention(768)\n",
        "\n",
        "# Create data for OPTICS\n",
        "concatenated_data = []\n",
        "for index, row in df.iterrows():\n",
        "    text_embedding = torch.tensor(row['text']) # Convert to PyTorch tensor\n",
        "    kg_embedding = torch.tensor(row['embedding'])  # Convert to PyTorch tensor\n",
        "\n",
        "    # Calculate context vector using SentenceAttention\n",
        "    context_vector = attention_model(text_embedding)\n",
        "\n",
        "    # Concatenate context vector and KG embedding\n",
        "    combined_embedding = torch.cat((context_vector, kg_embedding))\n",
        "\n",
        "    # Append to the data list\n",
        "    concatenated_data.append(combined_embedding.tolist())  # Convert Tensor to list for OPTICS\n",
        "\n",
        "# Perform OPTICS Clustering\n",
        "optics = OPTICS(min_samples=10, metric='euclidean')\n",
        "optics.fit(concatenated_data)\n",
        "\n",
        "# **3. Access Cluster Information**\n",
        "\n",
        "# Get cluster labels\n",
        "cluster_labels = optics.labels_\n",
        "\n",
        "# Get reachability distances (useful for understanding cluster structure)\n",
        "reachability_distances = optics.reachability_\n",
        "\n",
        "# Explore and utilize the clustering results as needed.\n",
        "# You might want to create a new column in your 'df' DataFrame\n",
        "# to store the cluster labels.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzBiZ3Q25KeG",
        "outputId": "91c3abfc-0867-4f21-c2f3-53b8a29cafde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c2ffaf5e34eb>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_embedding = torch.tensor(row['text']) # Convert to PyTorch tensor\n",
            "<ipython-input-7-c2ffaf5e34eb>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  kg_embedding = torch.tensor(row['embedding'])  # Convert to PyTorch tensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce dimensionality to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "data_reduced = pca.fit_transform(concatenated_data)\n",
        "\n",
        "# Plot the reduced data with cluster labels as colors\n",
        "plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=cluster_labels)\n",
        "plt.title('OPTICS Clusters (PCA Visualization)')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Da1tHEjLPFZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Reduce dimensionality to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2)\n",
        "data_reduced = tsne.fit_transform(concatenated_data)\n",
        "\n",
        "# Plot the reduced data with cluster labels as colors\n",
        "plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=cluster_labels)\n",
        "plt.title('OPTICS Clusters (t-SNE Visualization)')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UPA1KeyGPCve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(cluster_labels))\n",
        "print(cluster_labels.shape)\n",
        "df['cluster'] = cluster_labels"
      ],
      "metadata": {
        "id": "omoDFgYI8pqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.cluster import OPTICS\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Add cluster labels to the DataFrame\n",
        "df['cluster'] = cluster_labels\n",
        "\n",
        "# Separate text and kg embeddings for individual PCA\n",
        "text_embeddings = df['text'].tolist()  # Assuming 'text' contains lists of tensors\n",
        "kg_embeddings = df['embedding'].tolist()\n",
        "\n",
        "\n",
        "\n",
        "# Apply attention to text embeddings before PCA\n",
        "for i, text_embedding in enumerate(text_embeddings):\n",
        "    text_embedding = torch.tensor(text_embedding)\n",
        "    context_vector = attention_model(text_embedding)\n",
        "    text_reduced[i] = context_vector.detach().numpy()\n",
        "\n",
        "# PCA for text embeddings\n",
        "pca_text = PCA(n_components=1)\n",
        "text_reduced = pca_text.fit_transform(text_reduced)\n",
        "\n",
        "# PCA for KG embeddings\n",
        "pca_kg = PCA(n_components=1)\n",
        "kg_reduced = pca_kg.fit_transform(kg_embeddings)\n",
        "\n",
        "# Create new DataFrame for plotting\n",
        "plot_data = pd.DataFrame({\n",
        "    'text': text_reduced,\n",
        "    'embedding': kg_reduced,\n",
        "    'cluster': df['cluster']\n",
        "})\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(plot_data['text'], plot_data['embedding'], c=plot_data['cluster'])\n",
        "plt.title('Clusters with Text (X) and KG Embedding (Y)')\n",
        "plt.xlabel('Text Embedding (PCA)')\n",
        "plt.ylabel('KG Embedding (PCA)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hUN1gJBgQE7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"concept_embeddings (5).csv\")\n",
        "print(len(df))\n"
      ],
      "metadata": {
        "id": "SG-LGHklAQFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp concept_embeddings (5).csv /content/drive/MyDrive/WikiData/"
      ],
      "metadata": {
        "id": "6wefTtbXL851"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_dim = latent_size\n",
        "\n",
        "# Instantiate Generator and Discriminator\n",
        "generator = Generator(latent_size, 256).to(device)\n",
        "discriminator = Discriminator(latent_size).to(device)\n",
        "\n",
        "optimizer_generator = optim.Adam(generator.parameters())\n",
        "optimizer_discriminator = optim.Adam(discriminator.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()  # Standard for GANs\n",
        "\n",
        "# GAN Training Loop\n",
        "for epoch in range(epochs*10000):\n",
        "    # Generator\n",
        "    optimizer_generator.zero_grad()\n",
        "    noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "    #print(\"z_text: \",z_text.shape)\n",
        "    #print(z_text)\n",
        "    fake_kg_z = generator(z_text, noise)\n",
        "    #print(\"fake_kg_z: \",fake_kg_z.shape)\n",
        "    #print(fake_kg_z)\n",
        "    #print(\"real kg: \",kg_z.shape)\n",
        "    #print(kg_z)\n",
        "    d_on_fake = discriminator(fake_kg_z.detach())  # Detach to avoid backprop through discriminator\n",
        "    target_fake = torch.ones_like(d_on_fake)  # Labels for 'real' since we want to fool the discriminator\n",
        "    g_loss = criterion(d_on_fake, target_fake)\n",
        "    g_loss.backward()\n",
        "    optimizer_generator.step()\n",
        "\n",
        "    optimizer_discriminator.zero_grad()\n",
        "    d_real = discriminator(kg_z.detach())\n",
        "    target_real = torch.ones_like(d_real)\n",
        "    d_loss_real = criterion(d_real, target_real)\n",
        "\n",
        "    d_fake = discriminator(fake_kg_z.detach())\n",
        "    target_fake = torch.zeros_like(d_fake)\n",
        "    d_loss_fake = criterion(d_fake, target_fake)\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss.backward()\n",
        "    optimizer_discriminator.step()\n",
        "    if epoch % 1000 == 0:\n",
        "      print(f'Epoch {epoch+1}: Generator Loss - {g_loss:.4f} : Discriminator Loss - {d_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "PZJerd9osg41",
        "outputId": "e25ee114-e177-4b5a-b8fd-1dbd7a235a13"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Generator Loss - 0.6621 : Discriminator Loss - 1.3782\n",
            "Epoch 1001: Generator Loss - 1.8474 : Discriminator Loss - 0.6358\n",
            "Epoch 2001: Generator Loss - 1.9178 : Discriminator Loss - 0.6216\n",
            "Epoch 3001: Generator Loss - 2.1157 : Discriminator Loss - 0.6034\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b01e35e69801>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_loss_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# **Inference Function**\n",
        "def text_embedding(texts):\n",
        "    text_embeddings =[]\n",
        "    for query_text in input_texts:\n",
        "      inputs = tokenizer(query_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "      outputs = model1(**inputs)\n",
        "      text_embedding = outputs.last_hidden_state[:, 0, :].to(device)  # CLS Token Embedding\n",
        "      text_embeddings.append(text_embedding)\n",
        "    return torch.stack(text_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "# **Example Usage**\n",
        "input_texts = [\n",
        "    \"The vast grasslands of the [[African savanna]] support diverse wildlife.\",\n",
        "    \"[[animal]] is the opposite of [[bird]]', '[[animal]] is the opposite of [[human]]', '[[animal]] is the opposite of [[human plants]]', '[[animal]] is the opposite of [[man]]', '[[animal]] is the opposite of [[mineral]]'\",\n",
        "    \"[[Renaissance art]] emphasized [[classical forms]] and [[humanism]].\",\n",
        "    \"The intricate patterns of [[fractals]] reveal mathematical beauty.\",\n",
        "    \"The [[economic policies]] of the [[1980s]] had a lasting global impact.\",\n",
        "    \"[[Ancient mythology]] provides insights into the beliefs of early [[civilizations]].\",\n",
        "    \"The development of [[vaccines]] revolutionized modern medicine.\",\n",
        "    \"The [[human brain]] processes information with incredible complexity.\"\n",
        "    \"Name a large [[country]] in [[America]].\",\n",
        "    \"What is the name of a [[tall mountain range]] in [[Europe]]?\",\n",
        "    \"What is a soft-fleeced [[animal]] related to the camel?\",\n",
        "    \"What is the [[chemical symbol]] for [[gold]]?\",\n",
        "    \"What is the largest [[organ]] in the [[human body]]? \",\n",
        "    \"Which ancient [[civilization]] built the [[pyramids]]?\",\n",
        "    \"Which famous [[Greek philosopher]] was the [[teacher]] of [[Alexander the Great]]?\",\n",
        "    \"What is the [[capital city]] of [[France]]?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who was the 22nd president?\",\n",
        "    \"Who fought the bravest in the eastern front of WW2?\",\n",
        "    \"Who was the most sadistic figure in history?\",\n",
        "    \"Which straight public figure was actually homosexual?\",\n",
        "    \"Flying high in april and shot down in may\",\n",
        "    \"Gubernatorial woman\",\n",
        "    \"all your friends are being outcompeted in Piston Cup\",\n",
        "    \"The deep trenches of the [[Pacific Ocean]] are home to unique and mysterious creatures.\",\n",
        "    \"[[Mars]] is often referred to as the [[Red Planet]] due to its surface color.\",\n",
        "    \"The [[Renaissance period]] was marked by the revival of [[classical learning]] and wisdom.\",\n",
        "    \"The principles of [[quantum mechanics]] challenge our understanding of [[reality]].\",\n",
        "    \"The [[industrial revolution]] dramatically changed societies around the world.\",\n",
        "    \"Exploring ancient [[ruins]] offers a glimpse into past [[civilizations]].\",\n",
        "    \"The invention of the [[internet]] has transformed global communication.\",\n",
        "    \"The [[human genome]] contains the blueprint for building a person.\",\n",
        "    \"Name a country known for its [[ancient ruins]] in [[South America]].\",\n",
        "    \"What is the name of the [[deepest ocean trench]]?\",\n",
        "    \"What animal is known as the [[king of the jungle]]?\",\n",
        "    \"What is the [[chemical symbol]] for [[oxygen]]?\",\n",
        "    \"What is the smallest [[bone]] in the [[human body]]?\",\n",
        "    \"Which [[civilization]] invented [[writing]]?\",\n",
        "    \"Who was a famous [[Roman philosopher]] known for his meditations?\",\n",
        "    \"What is the [[capital city]] of [[Japan]]?\",\n",
        "    \"What is the tallest [[mountain]] in the world?\",\n",
        "    \"Who was the first [[woman]] in space?\",\n",
        "    \"Which country was the first to implement [[democracy]]?\",\n",
        "    \"Who is considered the father of [[modern physics]]?\",\n",
        "    \"What historical event is known as the [[Great Fire]] of London?\",\n",
        "    \"The phrase 'I think, therefore I am' was coined by which [[philosopher]]?\",\n",
        "    \"Which [[organism]] is known to live the longest?\",\n",
        "    \"What is the main ingredient in [[traditional Italian pizza dough]]?\",\n",
        "    \"Name a country in Africa that borders the Mediterranean Sea.\",\n",
        "    \"What is the capital of the smallest country in South America?\",\n",
        "    \"What large desert is found in northern China and Mongolia?\"\n",
        "]\n",
        "\n",
        "text_embeddings = text_embedding(input_texts)"
      ],
      "metadata": {
        "id": "p_JW4UcpeHv1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    z_text2, mu, logvar = textEncoder(text_embeddings)\n",
        "    print(\"textVAE: \",z_text2.shape)\n",
        "    print(z_text2)\n",
        "    noise = torch.randn(len(text_embeddings), noise_dim).to(device)\n",
        "    output = generator(z_text2, noise)\n",
        "    print(\"Generator: \",output.shape)\n",
        "    print(output)\n",
        "    output = kgDecoder(output)\n",
        "    print(\"kgVAE: \",output)\n",
        "    print(output.shape)\n",
        "    print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gcg_tJgYzZ-",
        "outputId": "bd4be699-596f-4b64-d0fe-9341ff39386f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 1, 2])\n",
            "tensor([[[ 0.0427,  0.8288]],\n",
            "\n",
            "        [[-1.2849, -0.8968]],\n",
            "\n",
            "        [[ 1.1223,  1.0121]],\n",
            "\n",
            "        [[-0.0598, -1.2572]],\n",
            "\n",
            "        [[-0.4035,  0.4646]],\n",
            "\n",
            "        [[ 0.9357, -1.3768]],\n",
            "\n",
            "        [[ 0.0158,  0.5987]],\n",
            "\n",
            "        [[ 0.5873,  0.4895]],\n",
            "\n",
            "        [[ 0.8552, -0.1122]],\n",
            "\n",
            "        [[ 0.1952, -0.2348]],\n",
            "\n",
            "        [[-0.6681,  0.0427]],\n",
            "\n",
            "        [[ 0.8981, -0.0984]],\n",
            "\n",
            "        [[-0.2466, -2.5536]],\n",
            "\n",
            "        [[-0.6721,  0.4880]],\n",
            "\n",
            "        [[ 0.6659, -0.1493]],\n",
            "\n",
            "        [[ 0.3012,  0.8938]],\n",
            "\n",
            "        [[-1.0954,  0.7342]],\n",
            "\n",
            "        [[-1.2155,  0.0941]],\n",
            "\n",
            "        [[-0.9578, -2.3597]],\n",
            "\n",
            "        [[ 0.6490,  0.4142]],\n",
            "\n",
            "        [[-1.6849, -0.0407]],\n",
            "\n",
            "        [[-0.2817, -2.3825]],\n",
            "\n",
            "        [[-0.6877,  0.0249]],\n",
            "\n",
            "        [[-1.4002,  0.0459]],\n",
            "\n",
            "        [[-0.3809, -0.9007]],\n",
            "\n",
            "        [[ 0.6658, -0.0985]],\n",
            "\n",
            "        [[ 0.4022,  0.3700]],\n",
            "\n",
            "        [[-1.3336,  0.8273]],\n",
            "\n",
            "        [[-2.7315, -0.6061]],\n",
            "\n",
            "        [[-2.2878, -0.8015]],\n",
            "\n",
            "        [[-0.6202, -0.9861]],\n",
            "\n",
            "        [[ 0.1062, -0.5924]],\n",
            "\n",
            "        [[-0.1524,  0.6348]],\n",
            "\n",
            "        [[ 0.4807, -0.1706]],\n",
            "\n",
            "        [[-1.1229, -0.0428]],\n",
            "\n",
            "        [[-0.6740, -0.7960]],\n",
            "\n",
            "        [[-1.9600, -1.8502]],\n",
            "\n",
            "        [[-0.8172,  1.4749]],\n",
            "\n",
            "        [[-0.2245,  0.1199]],\n",
            "\n",
            "        [[ 0.0451,  0.5193]],\n",
            "\n",
            "        [[ 0.6314, -0.6490]],\n",
            "\n",
            "        [[ 1.1497, -0.7029]],\n",
            "\n",
            "        [[-0.9284,  0.9075]],\n",
            "\n",
            "        [[-0.7205, -0.8610]],\n",
            "\n",
            "        [[-1.5132,  0.0801]],\n",
            "\n",
            "        [[-1.2063, -0.5547]],\n",
            "\n",
            "        [[-0.5825,  1.2774]],\n",
            "\n",
            "        [[ 0.2488, -0.1339]],\n",
            "\n",
            "        [[-0.5500, -0.7513]],\n",
            "\n",
            "        [[-0.6193,  0.5731]]], device='cuda:0')\n",
            "torch.Size([50, 1, 2])\n",
            "tensor([[[0.1002, 0.0463]],\n",
            "\n",
            "        [[0.1037, 0.0511]],\n",
            "\n",
            "        [[0.1028, 0.0498]],\n",
            "\n",
            "        [[0.1024, 0.0491]],\n",
            "\n",
            "        [[0.1034, 0.0506]],\n",
            "\n",
            "        [[0.1042, 0.0524]],\n",
            "\n",
            "        [[0.1032, 0.0513]],\n",
            "\n",
            "        [[0.1035, 0.0508]],\n",
            "\n",
            "        [[0.1028, 0.0508]],\n",
            "\n",
            "        [[0.1039, 0.0515]],\n",
            "\n",
            "        [[0.1033, 0.0506]],\n",
            "\n",
            "        [[0.1036, 0.0516]],\n",
            "\n",
            "        [[0.1041, 0.0519]],\n",
            "\n",
            "        [[0.1013, 0.0494]],\n",
            "\n",
            "        [[0.1001, 0.0491]],\n",
            "\n",
            "        [[0.1040, 0.0516]],\n",
            "\n",
            "        [[0.1017, 0.0482]],\n",
            "\n",
            "        [[0.1035, 0.0508]],\n",
            "\n",
            "        [[0.1031, 0.0511]],\n",
            "\n",
            "        [[0.1031, 0.0511]],\n",
            "\n",
            "        [[0.1012, 0.0493]],\n",
            "\n",
            "        [[0.1033, 0.0514]],\n",
            "\n",
            "        [[0.1032, 0.0513]],\n",
            "\n",
            "        [[0.1005, 0.0466]],\n",
            "\n",
            "        [[0.1044, 0.0524]],\n",
            "\n",
            "        [[0.1028, 0.0508]],\n",
            "\n",
            "        [[0.1038, 0.0518]],\n",
            "\n",
            "        [[0.1041, 0.0523]],\n",
            "\n",
            "        [[0.1033, 0.0505]],\n",
            "\n",
            "        [[0.1041, 0.0522]],\n",
            "\n",
            "        [[0.1030, 0.0500]],\n",
            "\n",
            "        [[0.1017, 0.0497]],\n",
            "\n",
            "        [[0.1031, 0.0502]],\n",
            "\n",
            "        [[0.1020, 0.0500]],\n",
            "\n",
            "        [[0.1022, 0.0489]],\n",
            "\n",
            "        [[0.1008, 0.0491]],\n",
            "\n",
            "        [[0.1020, 0.0499]],\n",
            "\n",
            "        [[0.1022, 0.0501]],\n",
            "\n",
            "        [[0.1029, 0.0499]],\n",
            "\n",
            "        [[0.1035, 0.0509]],\n",
            "\n",
            "        [[0.1014, 0.0495]],\n",
            "\n",
            "        [[0.1011, 0.0492]],\n",
            "\n",
            "        [[0.1031, 0.0503]],\n",
            "\n",
            "        [[0.1040, 0.0517]],\n",
            "\n",
            "        [[0.1016, 0.0496]],\n",
            "\n",
            "        [[0.1026, 0.0506]],\n",
            "\n",
            "        [[0.1043, 0.0524]],\n",
            "\n",
            "        [[0.1035, 0.0508]],\n",
            "\n",
            "        [[0.1030, 0.0501]],\n",
            "\n",
            "        [[0.1015, 0.0479]]], device='cuda:0')\n",
            "torch.Size([50, 1, 300])\n",
            "tensor([[[ 14.8829,  10.8679, -95.5058,  ...,  58.0258, -40.9445,  -7.5814]],\n",
            "\n",
            "        [[ 14.8947,  10.8768, -95.5824,  ...,  58.0724, -40.9773,  -7.5874]],\n",
            "\n",
            "        [[ 14.8917,  10.8744, -95.5624,  ...,  58.0602, -40.9687,  -7.5858]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 14.8941,  10.8763, -95.5783,  ...,  58.0699, -40.9755,  -7.5871]],\n",
            "\n",
            "        [[ 14.8925,  10.8751, -95.5678,  ...,  58.0635, -40.9710,  -7.5862]],\n",
            "\n",
            "        [[ 14.8871,  10.8710, -95.5330,  ...,  58.0423, -40.9561,  -7.5835]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgdGfutNFoWR",
        "outputId": "5b48499d-ec2a-4bc8-d7e7-bf4015d205d4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.8563,  0.5031, -5.1106,  ...,  3.1372, -2.0765, -0.6397]],\n",
            "\n",
            "        [[ 1.1766,  0.7233, -6.9593,  ...,  4.2933, -2.7650, -0.8817]],\n",
            "\n",
            "        [[ 1.1313,  0.6816, -6.6980,  ...,  4.1449, -2.6829, -0.8759]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.9828,  0.5775, -5.6951,  ...,  3.5325, -2.2821, -0.7345]],\n",
            "\n",
            "        [[ 0.7487,  0.4458, -4.5216,  ...,  2.7731, -1.8405, -0.5475]],\n",
            "\n",
            "        [[ 0.9645,  0.5989, -5.8459,  ...,  3.5890, -2.3421, -0.7158]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for embedding in output:\n",
        "  distances = F.cosine_similarity(embedding, torch.Tensor(embeddings).to(device), dim=-1)\n",
        "  # Find the top 5 closest entities\n",
        "  top_5_indices = torch.topk(distances, k=5).indices\n",
        "  print(f\"Query: {input_texts[i]}\")\n",
        "  print(\"Closest Entities:\")\n",
        "  i=i+1\n",
        "  for concept_idx in top_5_indices:\n",
        "    concept_label = vocabulary[concept_idx.item()]\n",
        "    distance = distances[concept_idx.item()].item()  # Get float value\n",
        "    print(f\"  - Concept Label: {concept_label} (Distance: {distance:.4f})\")\n",
        "    print(\"---\") # Separator between queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnHg5SpiehKl",
        "outputId": "101c375e-54a7-4369-ab44-95911e3e554f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: The vast grasslands of the [[African savanna]] support diverse wildlife.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8006)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7378)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6364)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6243)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6096)\n",
            "---\n",
            "Query: [[animal]] is the opposite of [[bird]]', '[[animal]] is the opposite of [[human]]', '[[animal]] is the opposite of [[human plants]]', '[[animal]] is the opposite of [[man]]', '[[animal]] is the opposite of [[mineral]]'\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8011)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7372)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6374)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6103)\n",
            "---\n",
            "Query: [[Renaissance art]] emphasized [[classical forms]] and [[humanism]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8012)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7368)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6373)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6253)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6109)\n",
            "---\n",
            "Query: The intricate patterns of [[fractals]] reveal mathematical beauty.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8006)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7379)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6365)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6242)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6096)\n",
            "---\n",
            "Query: The [[economic policies]] of the [[1980s]] had a lasting global impact.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8012)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7370)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6371)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6248)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6107)\n",
            "---\n",
            "Query: [[Ancient mythology]] provides insights into the beliefs of early [[civilizations]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7371)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6374)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6250)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6102)\n",
            "---\n",
            "Query: The development of [[vaccines]] revolutionized modern medicine.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8009)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7374)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6371)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6100)\n",
            "---\n",
            "Query: The [[human brain]] processes information with incredible complexity.Name a large [[country]] in [[America]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8008)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7376)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6366)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6244)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6099)\n",
            "---\n",
            "Query: What is the name of a [[tall mountain range]] in [[Europe]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8011)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7372)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6372)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6103)\n",
            "---\n",
            "Query: What is a soft-fleeced [[animal]] related to the camel?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8013)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7369)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6375)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6248)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6105)\n",
            "---\n",
            "Query: What is the [[chemical symbol]] for [[gold]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8008)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7375)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6370)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6100)\n",
            "---\n",
            "Query: What is the largest [[organ]] in the [[human body]]? \n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7373)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6369)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6103)\n",
            "---\n",
            "Query: Which ancient [[civilization]] built the [[pyramids]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8012)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7371)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6373)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6102)\n",
            "---\n",
            "Query: Which famous [[Greek philosopher]] was the [[teacher]] of [[Alexander the Great]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7371)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6370)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6250)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6106)\n",
            "---\n",
            "Query: What is the [[capital city]] of [[France]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8009)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7374)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6368)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6248)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6103)\n",
            "---\n",
            "Query: What is the capital of France?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8008)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7375)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6367)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6102)\n",
            "---\n",
            "Query: Who was the 22nd president?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8009)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7373)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6370)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6248)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6103)\n",
            "---\n",
            "Query: Who fought the bravest in the eastern front of WW2?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7376)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6368)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6100)\n",
            "---\n",
            "Query: Who was the most sadistic figure in history?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8008)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7374)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6371)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6100)\n",
            "---\n",
            "Query: Which straight public figure was actually homosexual?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8005)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7379)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6364)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6243)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6094)\n",
            "---\n",
            "Query: Flying high in april and shot down in may\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7377)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6366)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6244)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6095)\n",
            "---\n",
            "Query: Gubernatorial woman\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7372)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6368)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6248)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6104)\n",
            "---\n",
            "Query: all your friends are being outcompeted in Piston Cup\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8005)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7379)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6364)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6244)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6097)\n",
            "---\n",
            "Query: The deep trenches of the [[Pacific Ocean]] are home to unique and mysterious creatures.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8008)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7372)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6371)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6251)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6104)\n",
            "---\n",
            "Query: [[Mars]] is often referred to as the [[Red Planet]] due to its surface color.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7376)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6367)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6100)\n",
            "---\n",
            "Query: The [[Renaissance period]] was marked by the revival of [[classical learning]] and wisdom.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8008)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7373)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6368)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6249)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6103)\n",
            "---\n",
            "Query: The principles of [[quantum mechanics]] challenge our understanding of [[reality]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7372)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6369)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6104)\n",
            "---\n",
            "Query: The [[industrial revolution]] dramatically changed societies around the world.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7373)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6367)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6103)\n",
            "---\n",
            "Query: Exploring ancient [[ruins]] offers a glimpse into past [[civilizations]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7370)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6376)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6250)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6107)\n",
            "---\n",
            "Query: The invention of the [[internet]] has transformed global communication.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8011)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7370)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6370)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6250)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6107)\n",
            "---\n",
            "Query: The [[human genome]] contains the blueprint for building a person.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7371)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6369)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6249)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6106)\n",
            "---\n",
            "Query: Name a country known for its [[ancient ruins]] in [[South America]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8006)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7377)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6367)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6101)\n",
            "---\n",
            "Query: What is the name of the [[deepest ocean trench]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7372)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6370)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6248)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6106)\n",
            "---\n",
            "Query: What animal is known as the [[king of the jungle]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8009)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7373)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6371)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6249)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6106)\n",
            "---\n",
            "Query: What is the [[chemical symbol]] for [[oxygen]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7376)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6368)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6101)\n",
            "---\n",
            "Query: What is the smallest [[bone]] in the [[human body]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7378)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6366)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6243)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6097)\n",
            "---\n",
            "Query: Which [[civilization]] invented [[writing]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8008)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7375)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6368)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6245)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6100)\n",
            "---\n",
            "Query: Who was a famous [[Roman philosopher]] known for his meditations?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7375)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6367)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6102)\n",
            "---\n",
            "Query: What is the [[capital city]] of [[Japan]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8016)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7365)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6376)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6251)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6110)\n",
            "---\n",
            "Query: What is the tallest [[mountain]] in the world?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8013)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7367)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6375)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6250)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6107)\n",
            "---\n",
            "Query: Who was the first [[woman]] in space?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8006)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7377)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6366)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6098)\n",
            "---\n",
            "Query: Which country was the first to implement [[democracy]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8013)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7369)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6372)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6248)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6109)\n",
            "---\n",
            "Query: Who is considered the father of [[modern physics]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7372)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6367)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6248)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6104)\n",
            "---\n",
            "Query: What historical event is known as the [[Great Fire]] of London?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7378)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6366)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6242)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6097)\n",
            "---\n",
            "Query: The phrase 'I think, therefore I am' was coined by which [[philosopher]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8012)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7369)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6370)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6249)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6107)\n",
            "---\n",
            "Query: Which [[organism]] is known to live the longest?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8014)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7368)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6371)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6249)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6106)\n",
            "---\n",
            "Query: What is the main ingredient in [[traditional Italian pizza dough]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7376)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6367)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6247)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6101)\n",
            "---\n",
            "Query: Name a country in Africa that borders the Mediterranean Sea.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8010)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7373)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6371)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6246)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6103)\n",
            "---\n",
            "Query: What is the capital of the smallest country in South America?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8005)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7380)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6364)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6242)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6095)\n",
            "---\n",
            "Query: What large desert is found in northern China and Mongolia?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8007)\n",
            "---\n",
            "  - Concept Label: pt (Distance: 0.7377)\n",
            "---\n",
            "  - Concept Label: es (Distance: 0.6368)\n",
            "---\n",
            "  - Concept Label: ja (Distance: 0.6245)\n",
            "---\n",
            "  - Concept Label: je (Distance: 0.6100)\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(beta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6Uxhip_Xtq4",
        "outputId": "2058c210-d2ef-4580-d2c2-2cfea90e4b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZMKI_MHWJ8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data.dataset as dataset\n",
        "device = torch.device(\"cuda\")\n",
        "class WikiDataset(dataset.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Flatten text embeddings into a single tensor\n",
        "        text_embeddings = torch.cat([self.df.iloc[index]['text']]).float()\n",
        "\n",
        "        # Convert kg_embedding to a PyTorch tensor\n",
        "        kg_embedding = torch.tensor(self.df.iloc[index]['embedding']).float()\n",
        "        return text_embeddings, kg_embedding\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nrf2GZbcxrrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, BertTokenizer\n",
        "device = torch.device(\"cuda\")\n",
        "class SentenceAttention(nn.Module):\n",
        "    def __init__(self, attention_hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(attention_hidden_size, attention_hidden_size)\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "      # Transform x using a linear layer; output shape will be (sq, b, hidden_size)\n",
        "      x_transformed = self.linear(encoder_outputs)\n",
        "      # Step 2: Compute attention scores using softmax across the sequence dimension (sq)\n",
        "      # Attention scores shape: (sq, b, hidden_size) -> (b, sq, hidden_size) for softmax\n",
        "      x_transposed = x_transformed.transpose(0, 1)  # Transposing for softmax operation\n",
        "      attention_scores = F.softmax(x_transposed, dim=1)  # Applying softmax; shape remains (b, sq, hidden_size)\n",
        "\n",
        "      # Step 3: Apply attention scores to the original input tensor\n",
        "      # For weighted sum, first transpose x back: (sq, b, hidden_size) -> (b, sq, hidden_size)\n",
        "      x = encoder_outputs.transpose(0, 1)  # Transposing x to match attention_scores shape\n",
        "      # Compute the context vector as the weighted sum of the input vectors\n",
        "      # (b, sq, hidden_size) * (b, sq, hidden_size) -> (b, hidden_size) after summing over sq dimension\n",
        "      context_vector = torch.sum(attention_scores * x, dim=1)\n",
        "      return context_vector\n",
        "# **1. Text Encoder**\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, bert_embedding_size, encoder_hidden_size, latent_size):\n",
        "        super().__init__()\n",
        "        self.encoder_size = encoder_hidden_size\n",
        "        self.bert_size = bert_embedding_size\n",
        "        # Transformer Encoder Layer Configuration\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=bert_embedding_size,\n",
        "            nhead=8,  # Number of attention heads\n",
        "            dim_feedforward=encoder_hidden_size, # Dim. of feedforward network\n",
        "            batch_first=False\n",
        "        )\n",
        "\n",
        "        # Stack Multiple Transformer Layers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=5)\n",
        "        self.linear1 = nn.Linear(bert_embedding_size, encoder_hidden_size)\n",
        "        self.linear2 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear3 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear4 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear5 = nn.Linear(encoder_hidden_size, latent_size)\n",
        "        self.linear_mu = nn.Linear(latent_size, latent_size)\n",
        "        self.linear_logvar = nn.Linear(latent_size, latent_size)\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "      out = self.transformer_encoder(text_embeddings)\n",
        "      attention_layer = SentenceAttention(self.bert_size).to(device)  # Initialize attention\n",
        "      context = attention_layer(out)  # Apply attention\n",
        "      # Rest of your encoder layers\n",
        "      out = torch.relu(self.linear1(context))\n",
        "      out = torch.relu(self.linear2(out))\n",
        "      out = torch.relu(self.linear3(out))\n",
        "      out = torch.relu(self.linear4(out))\n",
        "      out = torch.relu(self.linear5(out))\n",
        "\n",
        "\n",
        "      mu = self.linear_mu(out)\n",
        "      logvar = self.linear_logvar(out)\n",
        "\n",
        "      # Reparameterization trick for backpropagation through sampling\n",
        "      std = torch.exp(0.5 * logvar)\n",
        "      eps = torch.randn_like(std)\n",
        "      z = eps.mul(std).add_(mu)  # Sample from the latent distribution\n",
        "      return z, mu, logvar\n",
        "\n",
        "# **2. Knowledge Graph Embedding Decoder**\n",
        "class KGDecoder(nn.Module):\n",
        "    def __init__(self, latent_size, decoder_hidden_size, kg_embedding_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(latent_size, decoder_hidden_size)\n",
        "        self.linear2 = nn.Linear(decoder_hidden_size, decoder_hidden_size)  # Additional layers\n",
        "        self.output_layer = nn.Linear(decoder_hidden_size, kg_embedding_size)\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = torch.relu(self.linear1(z))\n",
        "        out = torch.relu(self.linear2(out))\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, latent_size):\n",
        "         super().__init__()\n",
        "         self.linear1 = nn.Linear(latent_size, 128)\n",
        "         self.linear2 = nn.Linear(128, 64)\n",
        "         self.output_layer = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, z):\n",
        "         out = torch.relu(self.linear1(z))\n",
        "         out = torch.relu(self.linear2(out))\n",
        "         out = self.output_layer(out)\n",
        "         return out\n",
        "\n",
        "\n",
        "\n",
        "# **3. Model, Loss, and Training**\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, text_encoder, kg_decoder, beta=1.0, gamma=1.0):\n",
        "        super().__init__()\n",
        "        self.text_encoder = text_encoder\n",
        "        self.kg_decoder = kg_decoder\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        z, mu, logvar = self.text_encoder(text_embeddings)\n",
        "        kg_embeddings_pred = self.kg_decoder(z)\n",
        "        return kg_embeddings_pred, mu, logvar\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Modified Loss Function**\n",
        "def loss_function(kg_embeddings_pred, kg_embeddings,z, mu, logvar, beta=1.0, gamma=1.0):\n",
        "    # Cosine Similarity Calculation\n",
        "    cosine_similarity = F.cosine_similarity(kg_embeddings_pred, kg_embeddings, dim=-1)\n",
        "\n",
        "    # Cosine Distance (1 - Cosine Similarity)\n",
        "    cosine_distance = 1 - cosine_similarity\n",
        "\n",
        "    # MSE Calculation\n",
        "\n",
        "    reconstruction_loss = torch.mean(cosine_distance)\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Total Correlation (TC) Penalty\n",
        "    cov_z = torch.cov(z.T)  # Covariance matrix of the latent representations\n",
        "    tc_loss = (cov_z.diagonal() - cov_z).sum()  # Focus on off-diagonal elements\n",
        "    #print(\"rc: \",reconstruction_loss)\n",
        "    #print(\"kl: \",beta * kl_divergence)\n",
        "    #print(\"tc: \",gamma * tc_loss)\n",
        "    # Combined Loss\n",
        "    loss = reconstruction_loss + beta * kl_divergence + gamma * tc_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "\n",
        "\n",
        "def dynamic_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for dynamic batching, handling variable-length text sequences.\n",
        "\n",
        "    Args:\n",
        "        batch: A list of tuples (text_embeddings, kg_embedding) from the dataset.\n",
        "\n",
        "    Returns:\n",
        "        text_embeddings_padded: Padded tensor of text embeddings.\n",
        "        kg_embeddings: Stacked tensor of KG embeddings.\n",
        "        lengths: Original lengths of text sequences before padding.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort data by descending length of text sequences (for efficient padding)\n",
        "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "\n",
        "    # Unpack data\n",
        "    text_embeddings_list, kg_embeddings = zip(*batch)\n",
        "\n",
        "    # Pad text embeddings\n",
        "    text_embeddings_padded = pad_sequence(text_embeddings_list, batch_first=False)\n",
        "\n",
        "    # Stack KG embeddings directly\n",
        "    kg_embeddings = torch.stack(kg_embeddings)\n",
        "\n",
        "\n",
        "    return text_embeddings_padded, kg_embeddings\n",
        "\n",
        "\n",
        "\n",
        "dataset = WikiDataset(df)\n",
        "batch_size1 = 16\n",
        "# Create the DataLoader with dynamic batching\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size1,\n",
        "    shuffle=True,  # Shuffle before each epoch\n",
        "    collate_fn=dynamic_collate_fn\n",
        ")\n",
        "latent_size1=8\n",
        "# Instantiate models (adjust hyperparameters)\n",
        "text_encoder = TextEncoder(bert_embedding_size=768,  # If using BERT-base,\n",
        "                           encoder_hidden_size=64,\n",
        "                           latent_size=latent_size1)\n",
        "\n",
        "kg_decoder = KGDecoder(latent_size=latent_size1,\n",
        "                       decoder_hidden_size=64,\n",
        "                       kg_embedding_size=300)\n",
        "\n",
        "model = HybridVAE(text_encoder, kg_decoder,beta=0.75,gamma=0.000001).to(device)  # Clean instantiation\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# **Training Loop**\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.25, patience=10)\n",
        "\n",
        "discriminator = Discriminator(latent_size1).to(device)  # Assuming your Discriminator class is defined\n",
        "optimizer_discriminator = optim.Adam(discriminator.parameters())  # Or a suitable optimizer of your choice\n",
        "criterion_D = nn.BCEWithLogitsLoss()\n",
        "\n",
        "import torch\n",
        "\n",
        "def sample_from_marginals(z):\n",
        "    \"\"\"\n",
        "    Samples from marginal distributions with an emphasis on diverse cosine similarities.\n",
        "\n",
        "    Args:\n",
        "        z: The latent code tensor (batch_size, latent_size)\n",
        "\n",
        "    Returns:\n",
        "        marginal_samples: A tensor of the same shape as z, with samples encouraging\n",
        "                          a wider range of cosine similarities.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, latent_size = z.shape\n",
        "    marginal_means = z.mean(dim=0)\n",
        "\n",
        "    # Multiplier to increase the influence of cosine similarity\n",
        "    cosine_sim_multiplier = 0.25  # Adjust this as needed\n",
        "\n",
        "    marginal_samples = torch.zeros_like(z)\n",
        "    for i in range(latent_size):\n",
        "        # Similarities with the current dimension as the anchor\n",
        "        similarities = F.cosine_similarity(z[:, i].unsqueeze(1), z, dim=0)\n",
        "\n",
        "        # Increase the distance from similar samples\n",
        "        sim_adjusted = similarities * cosine_sim_multiplier\n",
        "        modified_std = torch.std(sim_adjusted)  # Std of adjusted similarities\n",
        "\n",
        "        marginal_samples[:, i] = torch.normal(marginal_means[i], modified_std)\n",
        "\n",
        "    return marginal_samples\n",
        "\n",
        "\n",
        "for epoch in range(500):\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training Phase\n",
        "    i = 0\n",
        "    k = 0\n",
        "    p = 0\n",
        "    for text_embeddings_packed, kg_embeddings in dataloader:\n",
        "        p=p+1\n",
        "        text_embeddings_packed = text_embeddings_packed.to(device)\n",
        "        kg_embeddings = kg_embeddings.to(device)\n",
        "        i=i+batch_size1\n",
        "        if i>=5000:\n",
        "          k=k+1\n",
        "          print(\"i: \",i*k)\n",
        "          i=0\n",
        "        optimizer.zero_grad()\n",
        "        # Encoding\n",
        "        z, mu, logvar = model.text_encoder(text_embeddings_packed)\n",
        "        # Decoding\n",
        "        kg_embeddings_pred = model.kg_decoder(z)\n",
        "\n",
        "        # Loss Calculation\n",
        "        loss = loss_function(kg_embeddings_pred, kg_embeddings, z, mu, logvar,model.beta,model.gamma)\n",
        "\n",
        "\n",
        "        for _ in range(20): # Train discriminator multiple times per encoder/decoder update\n",
        "          optimizer_discriminator.zero_grad()\n",
        "          # Real samples\n",
        "          real_samples = z.detach()  # Detach to avoid gradient backprop through encoder\n",
        "          d_real = discriminator(real_samples)\n",
        "          target_real = torch.ones_like(d_real)\n",
        "          # Fake samples (by sampling from marginals)\n",
        "          marginal_samples = sample_from_marginals(z)  # You'll need to implement this function\n",
        "          d_fake = discriminator(marginal_samples)\n",
        "          target_fake = torch.zeros_like(d_fake)\n",
        "          # Discriminator loss\n",
        "          d_loss = criterion_D(d_real, target_real)  + criterion_D(d_fake, target_fake)\n",
        "\n",
        "          d_loss.backward(retain_graph=True)\n",
        "          optimizer_discriminator.step()\n",
        "        # --- Adversarial Loss for VAE ---\n",
        "        d_on_z = discriminator(z)  # Discriminator's output on the encoded z\n",
        "        target_real_vae = torch.ones_like(d_on_z)  # Target \"real\" for the VAE\n",
        "        adversarial_loss = criterion_D(d_on_z, target_real_vae)\n",
        "\n",
        "        # Total VAE Loss\n",
        "        total_loss_VAE = loss +  adversarial_loss  # lambda_adv controls adversarial weighting\n",
        "        total_loss_VAE.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += total_loss_VAE.item()  # Add to epoch loss\n",
        "\n",
        "\n",
        "\n",
        "    # Print epoch information\n",
        "    epoch_loss = total_loss / p\n",
        "    scheduler.step(epoch_loss)\n",
        "    print(f'Epoch {epoch+1}: Training Loss - {epoch_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "bC5mZURe9mVy",
        "outputId": "349a0cfb-10b6-450b-e7c6-7e03ea85ce32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "<ipython-input-6-81ab89947052>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  kg_embedding = torch.tensor(self.df.iloc[index]['embedding']).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i:  5008\n",
            "i:  10016\n",
            "i:  15024\n",
            "i:  20032\n",
            "Epoch 1: Training Loss - 0.3579\n",
            "i:  5008\n",
            "i:  10016\n",
            "i:  15024\n",
            "i:  20032\n",
            "Epoch 2: Training Loss - 0.2686\n",
            "i:  5008\n",
            "i:  10016\n",
            "i:  15024\n",
            "i:  20032\n",
            "Epoch 3: Training Loss - 0.2701\n",
            "i:  5008\n",
            "i:  10016\n",
            "i:  15024\n",
            "i:  20032\n",
            "Epoch 4: Training Loss - 0.2735\n",
            "i:  5008\n",
            "i:  10016\n",
            "i:  15024\n",
            "i:  20032\n",
            "Epoch 5: Training Loss - 0.2775\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-260554ff6bfb>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_real\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mcriterion_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m           \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m           \u001b[0moptimizer_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# --- Adversarial Loss for VAE ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# **Inference Function**\n",
        "def find_closest_entities(model, input_texts, embeddings, vocabulary, tokenizer, bert_model):\n",
        "    \"\"\"\n",
        "    Finds the closest entities in ConceptNet embeddings for a list of input texts.\n",
        "\n",
        "    Args:\n",
        "        model (HybridVAE): The trained HybridVAE model.\n",
        "        input_texts (list): A list of input text descriptions.\n",
        "        embeddings (np.array): The ConceptNet embeddings array.\n",
        "        vocabulary (list): The vocabulary of concept labels.\n",
        "        tokenizer (BertTokenizer): The BERT tokenizer.\n",
        "        bert_model (BertModel): The BERT language model.\n",
        "\n",
        "    Returns:\n",
        "        None (Prints the results directly).\n",
        "    \"\"\"\n",
        "\n",
        "    for query_text in input_texts:\n",
        "        with torch.no_grad():\n",
        "            # Encode the text input\n",
        "            inputs = tokenizer(query_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            outputs = bert_model(**inputs)\n",
        "            text_embedding = outputs.last_hidden_state[:, 0, :].to(device)  # CLS Token Embedding\n",
        "\n",
        "            # Process through the text encoder\n",
        "            z, _, _ = model.text_encoder(text_embedding.unsqueeze(dim=0))  # Add a dimension for batch\n",
        "\n",
        "            # Decode to get a representation\n",
        "            predicted_embedding = model.kg_decoder(z).squeeze(dim=0)\n",
        "            print(\"Predicted Embedding (first 5 elements):\", predicted_embedding[:5])\n",
        "            # Calculate cosine similarities against ConceptNet embeddings\n",
        "            distances = F.cosine_similarity(predicted_embedding, torch.Tensor(embeddings).to(device), dim=-1)\n",
        "\n",
        "            # Find the top 5 closest entities\n",
        "            top_5_indices = torch.topk(distances, k=5).indices\n",
        "\n",
        "            print(f\"Query: {query_text}\")\n",
        "            print(\"Closest Entities:\")\n",
        "\n",
        "            for concept_idx in top_5_indices:\n",
        "                concept_label = vocabulary[concept_idx.item()]\n",
        "                distance = distances[concept_idx.item()].item()  # Get float value\n",
        "                print(f\"  - Concept Label: {concept_label} (Distance: {distance:.4f})\")\n",
        "            print(\"---\") # Separator between queries\n",
        "\n",
        "# **Example Usage**\n",
        "input_texts = [\n",
        "    \"The vast grasslands of the [[African savanna]] support diverse wildlife.\",\n",
        "    \"[[animal]] is the opposite of [[bird]]', '[[animal]] is the opposite of [[human]]', '[[animal]] is the opposite of [[human plants]]', '[[animal]] is the opposite of [[man]]', '[[animal]] is the opposite of [[mineral]]'\",\n",
        "    \"[[Renaissance art]] emphasized [[classical forms]] and [[humanism]].\",\n",
        "    \"The intricate patterns of [[fractals]] reveal mathematical beauty.\",\n",
        "    \"The [[economic policies]] of the [[1980s]] had a lasting global impact.\",\n",
        "    \"[[Ancient mythology]] provides insights into the beliefs of early [[civilizations]].\",\n",
        "    \"The development of [[vaccines]] revolutionized modern medicine.\",\n",
        "    \"The [[human brain]] processes information with incredible complexity.\"\n",
        "    \"Name a large [[country]] in [[America]].\",\n",
        "    \"What is the name of a [[tall mountain range]] in [[Europe]]?\",\n",
        "    \"What is a soft-fleeced [[animal]] related to the camel?\",\n",
        "    \"What is the [[chemical symbol]] for [[gold]]?\",\n",
        "    \"What is the largest [[organ]] in the [[human body]]? \",\n",
        "    \"Which ancient [[civilization]] built the [[pyramids]]?\",\n",
        "    \"Which famous [[Greek philosopher]] was the [[teacher]] of [[Alexander the Great]]?\",\n",
        "    \"What is the [[capital city]] of [[France]]?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who was the 22nd president?\",\n",
        "    \"Who fought the bravest in the eastern front of WW2?\",\n",
        "    \"Who was the most sadistic figure in history?\",\n",
        "    \"Which straight public figure was actually homosexual?\",\n",
        "    \"Flying high in april and shot down in may\",\n",
        "    \"Gubernatorial woman\",\n",
        "    \"all your friends are being outcompeted in Piston Cup\",\n",
        "    \"The deep trenches of the [[Pacific Ocean]] are home to unique and mysterious creatures.\",\n",
        "    \"[[Mars]] is often referred to as the [[Red Planet]] due to its surface color.\",\n",
        "    \"The [[Renaissance period]] was marked by the revival of [[classical learning]] and wisdom.\",\n",
        "    \"The principles of [[quantum mechanics]] challenge our understanding of [[reality]].\",\n",
        "    \"The [[industrial revolution]] dramatically changed societies around the world.\",\n",
        "    \"Exploring ancient [[ruins]] offers a glimpse into past [[civilizations]].\",\n",
        "    \"The invention of the [[internet]] has transformed global communication.\",\n",
        "    \"The [[human genome]] contains the blueprint for building a person.\",\n",
        "    \"Name a country known for its [[ancient ruins]] in [[South America]].\",\n",
        "    \"What is the name of the [[deepest ocean trench]]?\",\n",
        "    \"What animal is known as the [[king of the jungle]]?\",\n",
        "    \"What is the [[chemical symbol]] for [[oxygen]]?\",\n",
        "    \"What is the smallest [[bone]] in the [[human body]]?\",\n",
        "    \"Which [[civilization]] invented [[writing]]?\",\n",
        "    \"Who was a famous [[Roman philosopher]] known for his meditations?\",\n",
        "    \"What is the [[capital city]] of [[Japan]]?\",\n",
        "    \"What is the tallest [[mountain]] in the world?\",\n",
        "    \"Who was the first [[woman]] in space?\",\n",
        "    \"Which country was the first to implement [[democracy]]?\",\n",
        "    \"Who is considered the father of [[modern physics]]?\",\n",
        "    \"What historical event is known as the [[Great Fire]] of London?\",\n",
        "    \"The phrase 'I think, therefore I am' was coined by which [[philosopher]]?\",\n",
        "    \"Which [[organism]] is known to live the longest?\",\n",
        "    \"What is the main ingredient in [[traditional Italian pizza dough]]?\",\n",
        "    \"Name a country in Africa that borders the Mediterranean Sea.\",\n",
        "    \"What is the capital of the smallest country in South America?\",\n",
        "    \"What large desert is found in northern China and Mongolia?\"\n",
        "]\n",
        "\n",
        "find_closest_entities(model, input_texts, embeddings, vocabulary, tokenizer, model1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "587vTADoZc5T",
        "outputId": "c5a3e0ea-79a7-466d-f6bd-825c2cbace46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Embedding (first 5 elements): tensor([ 17.6128,   9.5389, -99.3306,  42.5649, -31.4522], device='cuda:0')\n",
            "Query: The vast grasslands of the [[African savanna]] support diverse wildlife.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.7203,   8.5225, -88.6922,  38.0044, -28.0769], device='cuda:0')\n",
            "Query: [[animal]] is the opposite of [[bird]]', '[[animal]] is the opposite of [[human]]', '[[animal]] is the opposite of [[human plants]]', '[[animal]] is the opposite of [[man]]', '[[animal]] is the opposite of [[mineral]]'\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.2575,   7.7351, -80.4746,  34.4836, -25.4743], device='cuda:0')\n",
            "Query: [[Renaissance art]] emphasized [[classical forms]] and [[humanism]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.3098,   8.3026, -86.3868,  37.0162, -27.3457], device='cuda:0')\n",
            "Query: The intricate patterns of [[fractals]] reveal mathematical beauty.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.2820,   7.7492, -80.6134,  34.5426, -25.5172], device='cuda:0')\n",
            "Query: The [[economic policies]] of the [[1980s]] had a lasting global impact.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.8255,   8.5792, -89.2807,  38.2576, -28.2634], device='cuda:0')\n",
            "Query: [[Ancient mythology]] provides insights into the beliefs of early [[civilizations]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.4668,   8.3852, -87.2657,  37.3938, -27.6255], device='cuda:0')\n",
            "Query: The development of [[vaccines]] revolutionized modern medicine.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.3542,   8.3260, -86.6338,  37.1224, -27.4246], device='cuda:0')\n",
            "Query: The [[human brain]] processes information with incredible complexity.Name a large [[country]] in [[America]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.0229,   8.6865, -90.3851,  38.7310, -28.6141], device='cuda:0')\n",
            "Query: What is the name of a [[tall mountain range]] in [[Europe]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.5340,   8.9586, -93.2664,  39.9663, -29.5291], device='cuda:0')\n",
            "Query: What is a soft-fleeced [[animal]] related to the camel?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.9148,   9.1628, -95.4073,  40.8841, -30.2082], device='cuda:0')\n",
            "Query: What is the [[chemical symbol]] for [[gold]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.8977,   8.0788, -84.0697,  36.0246, -26.6136], device='cuda:0')\n",
            "Query: What is the largest [[organ]] in the [[human body]]? \n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.5964,   8.4545, -87.9987,  37.7082, -27.8592], device='cuda:0')\n",
            "Query: Which ancient [[civilization]] built the [[pyramids]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.4959,   7.8640, -81.8127,  35.0559, -25.8969], device='cuda:0')\n",
            "Query: Which famous [[Greek philosopher]] was the [[teacher]] of [[Alexander the Great]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 17.4607,   9.4586, -98.4645,  42.1934, -31.1751], device='cuda:0')\n",
            "Query: What is the [[capital city]] of [[France]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.3157,   8.3048, -86.4138,  37.0285, -27.3549], device='cuda:0')\n",
            "Query: What is the capital of France?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 17.5749,   9.5216, -99.1065,  42.4680, -31.3782], device='cuda:0')\n",
            "Query: Who was the 22nd president?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.1409,   8.7477, -91.0574,  39.0183, -28.8279], device='cuda:0')\n",
            "Query: Who fought the bravest in the eastern front of WW2?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.7371,   9.0716, -94.4090,  40.4548, -29.8893], device='cuda:0')\n",
            "Query: Who was the most sadistic figure in history?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.2288,   8.2583, -85.9335,  36.8230, -27.2026], device='cuda:0')\n",
            "Query: Which straight public figure was actually homosexual?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.3629,   8.8690, -92.3000,  39.5515, -29.2213], device='cuda:0')\n",
            "Query: Flying high in april and shot down in may\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.2693,   7.7423, -80.5405,  34.5115, -25.4931], device='cuda:0')\n",
            "Query: Gubernatorial woman\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.7633,   8.0072, -83.3150,  35.7004, -26.3728], device='cuda:0')\n",
            "Query: all your friends are being outcompeted in Piston Cup\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.1477,   8.2141, -85.4805,  36.6292, -27.0613], device='cuda:0')\n",
            "Query: The deep trenches of the [[Pacific Ocean]] are home to unique and mysterious creatures.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.6370,   9.0154, -93.8442,  40.2128, -29.7101], device='cuda:0')\n",
            "Query: [[Mars]] is often referred to as the [[Red Planet]] due to its surface color.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.5204,   8.9513, -93.1852,  39.9315, -29.5031], device='cuda:0')\n",
            "Query: The [[Renaissance period]] was marked by the revival of [[classical learning]] and wisdom.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.6175,   7.9290, -82.4934,  35.3483, -26.1130], device='cuda:0')\n",
            "Query: The principles of [[quantum mechanics]] challenge our understanding of [[reality]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.8162,   8.0344, -83.6140,  35.8294, -26.4695], device='cuda:0')\n",
            "Query: The [[industrial revolution]] dramatically changed societies around the world.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 17.5538,   9.5051, -99.0013,  42.4245, -31.3487], device='cuda:0')\n",
            "Query: Exploring ancient [[ruins]] offers a glimpse into past [[civilizations]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.4473,   8.9126, -92.7766,  39.7556, -29.3725], device='cuda:0')\n",
            "Query: The invention of the [[internet]] has transformed global communication.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.2989,   8.8358, -91.9332,  39.3937, -29.1024], device='cuda:0')\n",
            "Query: The [[human genome]] contains the blueprint for building a person.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.7171,   8.5213, -88.6721,  37.9957, -28.0710], device='cuda:0')\n",
            "Query: Name a country known for its [[ancient ruins]] in [[South America]].\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.4123,   8.3569, -86.9659,  37.2659, -27.5314], device='cuda:0')\n",
            "Query: What is the name of the [[deepest ocean trench]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.1096,   8.7315, -90.8819,  38.9435, -28.7731], device='cuda:0')\n",
            "Query: What animal is known as the [[king of the jungle]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.6072,   7.9233, -82.4368,  35.3242, -26.0944], device='cuda:0')\n",
            "Query: What is the [[chemical symbol]] for [[oxygen]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.4077,   8.3540, -86.9350,  37.2531, -27.5218], device='cuda:0')\n",
            "Query: What is the smallest [[bone]] in the [[human body]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.7845,   8.5569, -89.0503,  38.1601, -28.1915], device='cuda:0')\n",
            "Query: Which [[civilization]] invented [[writing]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.9756,   8.6602, -90.1315,  38.6213, -28.5330], device='cuda:0')\n",
            "Query: Who was a famous [[Roman philosopher]] known for his meditations?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.8984,   9.1578, -95.3084,  40.8401, -30.1741], device='cuda:0')\n",
            "Query: What is the [[capital city]] of [[Japan]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.0153,   8.1428, -84.7304,  36.3081, -26.8222], device='cuda:0')\n",
            "Query: What is the tallest [[mountain]] in the world?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.9988,   8.6714, -90.2500,  38.6729, -28.5708], device='cuda:0')\n",
            "Query: Who was the first [[woman]] in space?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.4077,   7.8165, -81.3172,  34.8442, -25.7395], device='cuda:0')\n",
            "Query: Which country was the first to implement [[democracy]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.8504,   8.5927, -89.4242,  38.3191, -28.3100], device='cuda:0')\n",
            "Query: Who is considered the father of [[modern physics]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.4213,   8.3629, -87.0123,  37.2845, -27.5428], device='cuda:0')\n",
            "Query: What historical event is known as the [[Great Fire]] of London?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 15.1936,   8.2394, -85.7299,  36.7370, -27.1396], device='cuda:0')\n",
            "Query: The phrase 'I think, therefore I am' was coined by which [[philosopher]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.6790,   7.9614, -82.8415,  35.4979, -26.2243], device='cuda:0')\n",
            "Query: Which [[organism]] is known to live the longest?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.9793,   8.1238, -84.5298,  36.2220, -26.7589], device='cuda:0')\n",
            "Query: What is the main ingredient in [[traditional Italian pizza dough]]?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.6122,   7.9259, -82.4676,  35.3373, -26.1053], device='cuda:0')\n",
            "Query: Name a country in Africa that borders the Mediterranean Sea.\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 14.5998,   7.9198, -82.3948,  35.3057, -26.0814], device='cuda:0')\n",
            "Query: What is the capital of the smallest country in South America?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n",
            "Predicted Embedding (first 5 elements): tensor([ 16.3512,   8.8604, -92.2323,  39.5241, -29.2011], device='cuda:0')\n",
            "Query: What large desert is found in northern China and Mongolia?\n",
            "Closest Entities:\n",
            "  - Concept Label: en (Distance: 0.8151)\n",
            "  - Concept Label: pt (Distance: 0.7337)\n",
            "  - Concept Label: es (Distance: 0.6359)\n",
            "  - Concept Label: nut_quad (Distance: 0.6091)\n",
            "  - Concept Label: en_quad (Distance: 0.6049)\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoboaOLfI127"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# *** Placeholders for your 8 input queries ***\n",
        "input_queries = [\n",
        "    \"The vast grasslands of the [[African savanna]] support diverse wildlife.\",\n",
        "    \"[[animal]] is the opposite of [[bird]]', '[[animal]] is the opposite of [[human]]', '[[animal]] is the opposite of [[human plants]]', '[[animal]] is the opposite of [[man]]', '[[animal]] is the opposite of [[mineral]]'\",\n",
        "    \"[[Renaissance art]] emphasized [[classical forms]] and [[humanism]].\",\n",
        "    \"The intricate patterns of [[fractals]] reveal mathematical beauty.\",\n",
        "    \"The [[economic policies]] of the [[1980s]] had a lasting global impact.\",\n",
        "    \"[[Ancient mythology]] provides insights into the beliefs of early [[civilizations]].\",\n",
        "    \"The development of [[vaccines]] revolutionized modern medicine.\",\n",
        "    \"The [[human brain]] processes information with incredible complexity.\"\n",
        "    \"Name a large [[country]] in [[America]].\",\n",
        "    \"What is the name of a [[tall mountain range]] in [[Europe]]?\",\n",
        "    \"What is a soft-fleeced [[animal]] related to the camel?\",\n",
        "    \"What is the [[chemical symbol]] for [[gold]]?\",\n",
        "    \"What is the largest [[organ]] in the [[human body]]? \",\n",
        "    \"Which ancient [[civilization]] built the [[pyramids]]?\",\n",
        "    \"Which famous [[Greek philosopher]] was the [[teacher]] of [[Alexander the Great]]?\",\n",
        "    \"What is the [[capital city]] of [[France]]?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who was the 22nd president?\",\n",
        "    \"Who fought the bravest in the eastern front of WW2?\",\n",
        "    \"Who was the most sadistic figure in history?\",\n",
        "    \"Which straight public figure was actually homosexual?\",\n",
        "    \"Flying high in april and shot down in may\",\n",
        "    \"Gubernatorial woman\",\n",
        "    \"all your friends are being outcompeted in Piston Cup\",\n",
        "    \"The deep trenches of the [[Pacific Ocean]] are home to unique and mysterious creatures.\",\n",
        "    \"[[Mars]] is often referred to as the [[Red Planet]] due to its surface color.\",\n",
        "    \"The [[Renaissance period]] was marked by the revival of [[classical learning]] and wisdom.\",\n",
        "    \"The principles of [[quantum mechanics]] challenge our understanding of [[reality]].\",\n",
        "    \"The [[industrial revolution]] dramatically changed societies around the world.\",\n",
        "    \"Exploring ancient [[ruins]] offers a glimpse into past [[civilizations]].\",\n",
        "    \"The invention of the [[internet]] has transformed global communication.\",\n",
        "    \"The [[human genome]] contains the blueprint for building a person.\",\n",
        "    \"Name a country known for its [[ancient ruins]] in [[South America]].\",\n",
        "    \"What is the name of the [[deepest ocean trench]]?\",\n",
        "    \"What animal is known as the [[king of the jungle]]?\",\n",
        "    \"What is the [[chemical symbol]] for [[oxygen]]?\",\n",
        "    \"What is the smallest [[bone]] in the [[human body]]?\",\n",
        "    \"Which [[civilization]] invented [[writing]]?\",\n",
        "    \"Who was a famous [[Roman philosopher]] known for his meditations?\",\n",
        "    \"What is the [[capital city]] of [[Japan]]?\",\n",
        "    \"What is the tallest [[mountain]] in the world?\",\n",
        "    \"Who was the first [[woman]] in space?\",\n",
        "    \"Which country was the first to implement [[democracy]]?\",\n",
        "    \"Who is considered the father of [[modern physics]]?\",\n",
        "    \"What historical event is known as the [[Great Fire]] of London?\",\n",
        "    \"The phrase 'I think, therefore I am' was coined by which [[philosopher]]?\",\n",
        "    \"Which [[organism]] is known to live the longest?\",\n",
        "    \"What is the main ingredient in [[traditional Italian pizza dough]]?\",\n",
        "    \"Name a country in Africa that borders the Mediterranean Sea.\",\n",
        "    \"What is the capital of the smallest country in South America?\",\n",
        "    \"What large desert is found in northern China and Mongolia?\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Encode input queries using BERT and HybridVAE\n",
        "\n",
        "with torch.no_grad():\n",
        "  encoded_queries = []\n",
        "  max_length = 0  # Initialize max_length\n",
        "  for query in input_queries:\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
        "    attention_mask = inputs.pop('attention_mask')\n",
        "    max_length = max(max_length, attention_mask.shape[-1])\n",
        "\n",
        "    outputs = model1(**inputs)\n",
        "    token_embeddings = outputs.last_hidden_state[:,0,:]\n",
        "    #print(token_embeddings.shape)\n",
        "    #print(token_embeddings.squeeze(0).shape)\n",
        "    encoded_queries.append(token_embeddings)\n",
        "  padded_queries = pad_sequence(encoded_queries, batch_first=False, padding_value=0)\n",
        "  print(padded_queries)\n",
        "  text_embeddings = padded_queries\n",
        "  print(text_embeddings.shape)\n",
        "\n",
        "\n",
        "  # VAE Encoding\n",
        "  latent_embeddings, _, _  = model.text_encoder(text_embeddings)\n",
        "  print(latent_embeddings.shape)\n",
        "\n",
        "  # VAE Decoding\n",
        "  kg_embeddings = model.kg_decoder(latent_embeddings)\n",
        "  kg_embeddings_np = kg_embeddings.numpy()\n",
        "\n",
        "print(kg_embeddings_np.shape)\n",
        "def calculate_cosine_similarity(emb, concept_embedding):\n",
        "    #print(\"concept: \",concept_embedding)\n",
        "    dot_product = np.dot(emb, concept_embedding)\n",
        "    magnitude_emb = np.linalg.norm(emb)\n",
        "    magnitude_concept_embedding = np.linalg.norm(concept_embedding)\n",
        "    cos_sim = dot_product / (magnitude_emb * magnitude_concept_embedding)\n",
        "    return cos_sim\n",
        "\n",
        "# Find nearest neighbors in ConceptNet embeddings\n",
        "for query_idx, query_embedding in enumerate(kg_embeddings_np): # Directly get query_embedding\n",
        "\n",
        "    distances = []\n",
        "    for idx, concept_embedding in enumerate(embeddings):\n",
        "        cos_sim = calculate_cosine_similarity(query_embedding, concept_embedding)  # Use query_embedding\n",
        "        distances.append(cos_sim)\n",
        "\n",
        "    top_5_indices = np.argsort(distances)[-5:]\n",
        "\n",
        "    print(f\"Query: {input_queries[query_idx]}\")\n",
        "    print(\"Closest Entities:\")\n",
        "\n",
        "    for concept_idx in top_5_indices:\n",
        "        concept_label = vocabulary[concept_idx]\n",
        "        distance = distances[concept_idx]\n",
        "        print(f\"  - Concept Label: {concept_label} (Distance: {distance:.4f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYUt0G0X_-kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "latent_embeddings_2d = TSNE.fit_transform(latent_embeddings)\n",
        "\n",
        "# Visualize using matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(latent_embeddings_2d[:, 0], latent_embeddings_2d[:, 1])\n",
        "\n",
        "# Annotate with the original text (optional)\n",
        "for i, query in enumerate(input_queries):\n",
        "    plt.annotate(query[:30] + '...', (latent_embeddings_2d[i, 0], latent_embeddings_2d[i, 1]))\n",
        "\n",
        "plt.title('t-SNE Visualization of Latent Space')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m4bvjyFa1iIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Name a large [[country]] in [[America]].\",\n",
        "    \"What is the name of a [[tall mountain range]] in [[Europe]]?\",\n",
        "    \"What is a soft-fleeced [[animal]] related to the camel?\",\n",
        "    \"What is the [[chemical symbol]] for [[gold]]?\",\n",
        "    \"What is the largest [[organ]] in the [[human body]]? \",\n",
        "    \"Which ancient [[civilization]] built the [[pyramids]]?\",\n",
        "    \"Which famous [[Greek philosopher]] was the [[teacher]] of [[Alexander the Great]]?\",\n",
        "    \"What is the [[capital city]] of [[France]]?\""
      ],
      "metadata": {
        "id": "gp3Z6NQpuS4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\"What is the capital of France?\",\n",
        "    \"Who was the 22nd president?\",\n",
        "    \"Who fought the bravest in the eastern front of WW2?\",\n",
        "    \"Who was the most sadistic figure in history?\",\n",
        "    \"Which straight public figure was actually homosexual?\",\n",
        "    \"Flying high in april and shot down in may\",\n",
        "    \"Gubernatorial woman\",\n",
        "    \"all your friends are being outcompeted in Piston Cup\""
      ],
      "metadata": {
        "id": "XGh8UXSOuR5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# **1. Text Encoder**\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, bert_embedding_size, lstm_hidden_size, encoder_hidden_size, latent_size):\n",
        "        super().__init__()\n",
        "        self.lstm1 = nn.LSTM(bert_embedding_size, lstm_hidden_size)\n",
        "        self.lstm2 = nn.LSTM(lstm_hidden_size, lstm_hidden_size)\n",
        "        self.lstm3 = nn.LSTM(lstm_hidden_size, lstm_hidden_size)\n",
        "        self.lstm4 = nn.LSTM(lstm_hidden_size, lstm_hidden_size)\n",
        "        self.lstm5 = nn.LSTM(lstm_hidden_size, lstm_hidden_size)\n",
        "\n",
        "        self.linear1 = nn.Linear(lstm_hidden_size, encoder_hidden_size)\n",
        "        self.linear2 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear3 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear4 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear5 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear6 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear7 = nn.Linear(encoder_hidden_size, encoder_hidden_size)\n",
        "        self.linear8 = nn.Linear(encoder_hidden_size, latent_size)\n",
        "        self.linear_mu = nn.Linear(latent_size, latent_size)\n",
        "        self.linear_logvar = nn.Linear(latent_size, latent_size)\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        out, hidden = self.lstm1(text_embeddings)\n",
        "        out, hidden = self.lstm2(out, hidden)    # Pass to second lstm\n",
        "        out, hidden = self.lstm3(out, hidden)\n",
        "        out, hidden = self.lstm4(out, hidden)\n",
        "        out, hidden = self.lstm5(out, hidden)\n",
        "        out = hidden[0][-1]\n",
        "\n",
        "        out = torch.relu(self.linear1(out))  # Encoder layers\n",
        "        out = torch.relu(self.linear2(out))\n",
        "        out = torch.relu(self.linear3(out))\n",
        "        out = torch.relu(self.linear4(out))\n",
        "        out = torch.relu(self.linear5(out))\n",
        "        out = torch.relu(self.linear6(out))\n",
        "        out = torch.relu(self.linear7(out))\n",
        "        out = torch.relu(self.linear8(out))\n",
        "\n",
        "\n",
        "        mu = self.linear_mu(out)\n",
        "        logvar = self.linear_logvar(out)\n",
        "\n",
        "        # Reparameterization trick for backpropagation through sampling\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = eps.mul(std).add_(mu)  # Sample from the latent distribution\n",
        "        return z, mu, logvar\n",
        "\n",
        "# **2. Knowledge Graph Embedding Decoder**\n",
        "class KGDecoder(nn.Module):\n",
        "    def __init__(self, latent_size, decoder_hidden_size, kg_embedding_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(latent_size, decoder_hidden_size)\n",
        "        self.linear2 = nn.Linear(decoder_hidden_size, decoder_hidden_size)  # Additional layers\n",
        "        self.linear3 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.linear4 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.linear5 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.linear6 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.linear7 = nn.Linear(decoder_hidden_size, decoder_hidden_size)  # Additional layers\n",
        "        self.linear8 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.linear9 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.linear10 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.linear11 = nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
        "        self.output_layer = nn.Linear(decoder_hidden_size, kg_embedding_size)\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = torch.relu(self.linear1(z))\n",
        "        out = torch.relu(self.linear2(out))\n",
        "        out = torch.relu(self.linear3(out))\n",
        "        out = torch.relu(self.linear4(out))\n",
        "        out = torch.relu(self.linear5(out))\n",
        "        out = torch.relu(self.linear6(out))\n",
        "        out = torch.relu(self.linear7(out))\n",
        "        out = torch.relu(self.linear8(out))\n",
        "        out = torch.relu(self.linear9(out))\n",
        "        out = torch.relu(self.linear10(out))\n",
        "        out = torch.relu(self.linear11(out))\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# **3. Model, Loss, and Training**\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, text_encoder, kg_decoder):\n",
        "        super().__init__()\n",
        "        self.text_encoder = text_encoder\n",
        "        self.kg_decoder = kg_decoder\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        z, mu, logvar = self.text_encoder(text_embeddings)\n",
        "        kg_embeddings_pred = self.kg_decoder(z)\n",
        "        return kg_embeddings_pred, mu, logvar\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Modified Loss Function**\n",
        "def loss_function(kg_embeddings_pred, kg_embeddings, mu, logvar, lengths):\n",
        "    kg_embeddings_pred = kg_embeddings_pred[0]\n",
        "    reconstruction_losses = F.mse_loss(kg_embeddings_pred, kg_embeddings, reduction='none')\n",
        "    reconstruction_loss = torch.sum(reconstruction_losses * lengths.float().unsqueeze(1)) / torch.sum(lengths)  # Weighted average\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return reconstruction_loss + kl_divergence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_batch_generator(dataset, batch_size):\n",
        "    batch = []\n",
        "    for i in range(len(dataset)):\n",
        "        sample = dataset[i]\n",
        "        text_embeddings, kg_embedding = sample[0], sample[1]\n",
        "        batch.append((text_embeddings.clone().detach(), kg_embedding))\n",
        "        if len(batch) == batch_size:\n",
        "            lengths = torch.tensor([len(x) for x, _ in batch])\n",
        "            # Packing logic\n",
        "            text_embeddings_packed = pad_sequence(\n",
        "                [x[0] for x in batch],  # Extract text embeddings\n",
        "                batch_first=False\n",
        "            )\n",
        "            kg_embeddings = torch.stack([y for _, y in batch])\n",
        "            yield text_embeddings_packed, kg_embeddings, lengths\n",
        "            batch = []\n",
        "\n",
        "    # Yield the last batch if it has less than batch_size elements\n",
        "    if batch:\n",
        "        lengths = torch.tensor([len(x) for x, _ in batch])\n",
        "        text_embeddings_packed = pad_sequence(\n",
        "                [x[0] for x in batch],  # Extract text embeddings\n",
        "                batch_first=True\n",
        "            )\n",
        "        kg_embeddings = torch.stack([y for _, y in batch])\n",
        "        yield text_embeddings_packed, kg_embeddings, lengths\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate models (adjust hyperparameters)\n",
        "text_encoder = TextEncoder(bert_embedding_size=768,  # If using BERT-base\n",
        "                           lstm_hidden_size=256,\n",
        "                           encoder_hidden_size=128,\n",
        "                           latent_size=32)\n",
        "\n",
        "kg_decoder = KGDecoder(latent_size=32,\n",
        "                       decoder_hidden_size=128,\n",
        "                       kg_embedding_size=300)\n",
        "\n",
        "model = HybridVAE(text_encoder, kg_decoder)  # Clean instantiation\n",
        "\n",
        "\n",
        "# **Training Loop**\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(500):\n",
        "    total_loss=0\n",
        "    for text_embeddings_packed, kg_embeddings, lengths in custom_batch_generator(WikiDataset(df), batch_size=8):\n",
        "        optimizer.zero_grad()\n",
        "        # Encoding\n",
        "        z, mu, logvar = model.text_encoder(text_embeddings_packed)\n",
        "\n",
        "        # Decoding\n",
        "        kg_embeddings_pred = model.kg_decoder(z)\n",
        "\n",
        "        # Loss Calculation\n",
        "        loss = loss_function(kg_embeddings_pred, kg_embeddings, z, mu, lengths)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()  # Add to epoch loss\n",
        "    # Print epoch information\n",
        "    epoch_loss = total_loss / 1000\n",
        "    print(f'Epoch {epoch+1}: Loss - {epoch_loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "ur3WU4-NuNnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yTXjn2DyyHYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gx_GFDYyyGWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuppcXckq5ao"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ecb8779bbb94ba883c4c6fbd28cc2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bb6669d3fe3400e8bb5db37a495cbb6",
              "IPY_MODEL_6e4165dd23684694b627ffa20dd24877",
              "IPY_MODEL_a0f60da9b416494cbdeeff225967a0a1"
            ],
            "layout": "IPY_MODEL_33f7a97c49b04810ad25e5e2f2042376"
          }
        },
        "9bb6669d3fe3400e8bb5db37a495cbb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66425a4e589462ab15a2ca575e2fb34",
            "placeholder": "​",
            "style": "IPY_MODEL_d15e687e17984d31b4edb1cb0d844533",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6e4165dd23684694b627ffa20dd24877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0219b2dda8b347108da726f092ae4932",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33d969cdfbd34a72950c5467a986b6da",
            "value": 48
          }
        },
        "a0f60da9b416494cbdeeff225967a0a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82da9a74216944a495d6eb495b4aa4ab",
            "placeholder": "​",
            "style": "IPY_MODEL_2f84e91d52034ea59d9ab6fd34b1877e",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.83kB/s]"
          }
        },
        "33f7a97c49b04810ad25e5e2f2042376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66425a4e589462ab15a2ca575e2fb34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d15e687e17984d31b4edb1cb0d844533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0219b2dda8b347108da726f092ae4932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d969cdfbd34a72950c5467a986b6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82da9a74216944a495d6eb495b4aa4ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f84e91d52034ea59d9ab6fd34b1877e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1e52fa437ce4a909535c968f433e3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63e10ddaafb04328911c311da0e47253",
              "IPY_MODEL_574167f653b4416fae42f0bcc94d350b",
              "IPY_MODEL_842d640f51e645c9b51ca168bc901f4e"
            ],
            "layout": "IPY_MODEL_cf444aa224f74b5fbefc7f07f6c8af59"
          }
        },
        "63e10ddaafb04328911c311da0e47253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_388ba9f96d234418949a06dedebc0d33",
            "placeholder": "​",
            "style": "IPY_MODEL_51a97f917b2e4460898bbdbf74f1187a",
            "value": "config.json: 100%"
          }
        },
        "574167f653b4416fae42f0bcc94d350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38248d6be2104ebeaf16f65db27cf91f",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15276ec0c4964e2a84c66bcc99fb1b6c",
            "value": 570
          }
        },
        "842d640f51e645c9b51ca168bc901f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c4f6ff5070843ec8b57e3227dfbf3eb",
            "placeholder": "​",
            "style": "IPY_MODEL_3603428fb61148ef858439229d2d03e6",
            "value": " 570/570 [00:00&lt;00:00, 51.7kB/s]"
          }
        },
        "cf444aa224f74b5fbefc7f07f6c8af59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "388ba9f96d234418949a06dedebc0d33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51a97f917b2e4460898bbdbf74f1187a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38248d6be2104ebeaf16f65db27cf91f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15276ec0c4964e2a84c66bcc99fb1b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c4f6ff5070843ec8b57e3227dfbf3eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3603428fb61148ef858439229d2d03e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c1352e83dd740c59e2cd341762db5bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9251a55054ac4bbbbe7b00fb91086862",
              "IPY_MODEL_28bd3c39e0b84a01972f6336cbca0f8c",
              "IPY_MODEL_bb9ac41a30c444f2993dd7b9dc25c6c8"
            ],
            "layout": "IPY_MODEL_e3eaf0ab17bf47ce94c51827c9742fa5"
          }
        },
        "9251a55054ac4bbbbe7b00fb91086862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c72b603b0a4f939f0dbcf1b0900fcf",
            "placeholder": "​",
            "style": "IPY_MODEL_e0f14474d77649e5b51d2ed48e0a17bc",
            "value": "vocab.txt: 100%"
          }
        },
        "28bd3c39e0b84a01972f6336cbca0f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74ef5c39d95a4ef98a5fa870a997be57",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_704711b3453746878b1970139c6f3a41",
            "value": 231508
          }
        },
        "bb9ac41a30c444f2993dd7b9dc25c6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c994b44bfc924a1cbe7dfc83448df189",
            "placeholder": "​",
            "style": "IPY_MODEL_b19c88f3e6564b7eb434adb218a475f7",
            "value": " 232k/232k [00:00&lt;00:00, 13.6MB/s]"
          }
        },
        "e3eaf0ab17bf47ce94c51827c9742fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8c72b603b0a4f939f0dbcf1b0900fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0f14474d77649e5b51d2ed48e0a17bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74ef5c39d95a4ef98a5fa870a997be57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "704711b3453746878b1970139c6f3a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c994b44bfc924a1cbe7dfc83448df189": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b19c88f3e6564b7eb434adb218a475f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a7ec4560e934f498395ac3fe9caf20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57d92d8b3a1044a9a4e333305a21d257",
              "IPY_MODEL_ad5b917e93f94bca80df5a7dccdc9006",
              "IPY_MODEL_376ab5a579894e24a99acb1e0c35de72"
            ],
            "layout": "IPY_MODEL_8c1e4e565e3c4a9c961bb0c4f0352142"
          }
        },
        "57d92d8b3a1044a9a4e333305a21d257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61f5040ba802493abceb77e7e3ae1527",
            "placeholder": "​",
            "style": "IPY_MODEL_6034673414984180a39be60f51d48047",
            "value": "tokenizer.json: 100%"
          }
        },
        "ad5b917e93f94bca80df5a7dccdc9006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c11ad45336c0447787e96de64f749bbe",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52e137360568400398b49076bf066954",
            "value": 466062
          }
        },
        "376ab5a579894e24a99acb1e0c35de72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a338674f9a8841b5a2ff85dd9f574637",
            "placeholder": "​",
            "style": "IPY_MODEL_5bf72b6152ee4b5fa1f944a55b2a9278",
            "value": " 466k/466k [00:00&lt;00:00, 29.8MB/s]"
          }
        },
        "8c1e4e565e3c4a9c961bb0c4f0352142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61f5040ba802493abceb77e7e3ae1527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6034673414984180a39be60f51d48047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c11ad45336c0447787e96de64f749bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52e137360568400398b49076bf066954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a338674f9a8841b5a2ff85dd9f574637": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf72b6152ee4b5fa1f944a55b2a9278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b135076b2331409e9071d2980e6c5ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db2b454548e0474aa2c14c1945762436",
              "IPY_MODEL_1de095d6f91049d889502d85c2765721",
              "IPY_MODEL_224a7b23b1564bf5a1f072f78da018ba"
            ],
            "layout": "IPY_MODEL_dde637bb9d1043bebbf9648eac1524c5"
          }
        },
        "db2b454548e0474aa2c14c1945762436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cfa349d089d43b9954fdf536d6c86dc",
            "placeholder": "​",
            "style": "IPY_MODEL_dee1c15d77154add91e3517209a20317",
            "value": "model.safetensors: 100%"
          }
        },
        "1de095d6f91049d889502d85c2765721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce1698ce20944f1b9be4ea2b87cc1e9",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_648260ad5bcc451ca3fdeddf5d5e530c",
            "value": 440449768
          }
        },
        "224a7b23b1564bf5a1f072f78da018ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a7dafe10f844779afd609de299a8ff",
            "placeholder": "​",
            "style": "IPY_MODEL_875aad3566a445f39173a078a9cbfbb0",
            "value": " 440M/440M [00:01&lt;00:00, 431MB/s]"
          }
        },
        "dde637bb9d1043bebbf9648eac1524c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cfa349d089d43b9954fdf536d6c86dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee1c15d77154add91e3517209a20317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ce1698ce20944f1b9be4ea2b87cc1e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "648260ad5bcc451ca3fdeddf5d5e530c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01a7dafe10f844779afd609de299a8ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "875aad3566a445f39173a078a9cbfbb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}